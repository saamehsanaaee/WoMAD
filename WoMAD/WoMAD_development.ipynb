{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvT2KwSvKHe"
      },
      "source": [
        "# WoMAD Development Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIyqEvZkvTQ5"
      },
      "source": [
        "### Setup and Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "collapsed": true,
        "id": "ByjkqCGNvdMc",
        "outputId": "7ff48f00-10b3-4200-9d11-e1e3e4db82d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_spline_conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'fsspec' has no attribute 'utils' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-325520387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPyG_Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_debug_enabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mppr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ppr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_train_test_split_edges\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split_edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minfluence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtotal_influence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/influence.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mk_hop_subgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhetero_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtemporal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTemporalData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRocksDatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseparate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mIndexType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtxt_array\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_txt_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_txt_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_tu_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplanetoid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_planetoid_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mply\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_ply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/io/txt_array.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mavailable_compressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_fs_token_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_to_fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFSTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/compression.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mregister_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munzip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/compression.py\u001b[0m in \u001b[0;36mregister_compression\u001b[0;34m(name, callback, extensions, force)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Duplicate compression file extension: {ext} ({name})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'fsspec' has no attribute 'utils' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "# Install basic dependencies\n",
        "!pip install --quiet requests nilearn nibabel brainspace numpy pandas scikit-learn torch torch-geometric scipy rich>=13.5.2\n",
        "!pip install --quiet optuna\n",
        "\n",
        "# Install torch_geometric and its dependencies\n",
        "!pip install --quiet torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
        "\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import zipfile\n",
        "from typing import List, Dict, Tuple, Any, Callable\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.data as PyG_Data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "import shap   # For result interp module\n",
        "import pytest # For TESTS module\n",
        "import optuna # For hyperparameter module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0f-lhxivPtZ",
        "outputId": "a984647f-f22b-44b2-eb70-5a97db01cb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Project Paths:\n",
        "import os\n",
        "from google.colab import drive; drive.mount('/content/drive') # Temporary for Colab\n",
        "project_root = \"/content/drive/MyDrive/WoMAD/WoMAD/Notebooks/data\" # Google Colab-specific (different in the .py files and the repo)\n",
        "\n",
        "unprocessed_path = os.path.join(project_root, \"HCP_zipped\")\n",
        "processed_path   = os.path.join(project_root, \"processed\")\n",
        "model_ready_path = os.path.join(project_root, \"model_ready\")\n",
        "\n",
        "sub_list_txt_path = os.path.join(project_root, \"full_3T_task_subjects.txt\")\n",
        "\n",
        "# WoMAD-specific variables:\n",
        "target_tasks = [\"WM\", \"EMOTION\", \"LANGUAGE\"]\n",
        "\n",
        "target_subtasks = {\n",
        "    \"WM\"      : [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
        "                 \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"],\n",
        "    \"EMOTION\" : [\"fear\", \"neut\"],\n",
        "    \"LANGUAGE\": [\"math\", \"story\"],\n",
        "}\n",
        "\n",
        "TR = 0.72\n",
        "\n",
        "rest_tasks    = [\"REST1\", \"REST2\"]\n",
        "run_direction = [\"LR\"   , \"RL\"]\n",
        "\n",
        "# Subjects with full 3T imaging protocol completed:\n",
        "full_3T_task_subjects = []\n",
        "\n",
        "with open(sub_list_txt_path, \"r\") as file:\n",
        "    raw_list = file.read()\n",
        "    str_list = raw_list.strip().split(\",\")\n",
        "    num_list = [int(subID.strip()) for subID in str_list if subID.strip()]\n",
        "\n",
        "full_3T_task_subjects = num_list\n",
        "\n",
        "# Model config dictionaries\n",
        "lstm_config = {\n",
        "    \"hidden_size\" : 128,\n",
        "    \"num_layers\"  :   2,\n",
        "    \"dropout\"     : 0.2\n",
        "}\n",
        "\n",
        "fusion_config = {\n",
        "    \"total_input_feats\" : 736, # Temporary: 360 parcels, 128 from the LSTM, and 248 guessing from the 4D network\n",
        "    \"hidden_size\"       : 128\n",
        "}\n",
        "\n",
        "training_loss_weights = {\n",
        "    \"overall_loss_weight\" : 0.5,\n",
        "    \"node_loss_weight\"    : 0.5\n",
        "}\n",
        "\n",
        "# Randomly selected subject list for the pilot:\n",
        "pilot_subjects = [283543, 180937, 379657, 145632, 100206, 270332, 707749, 454140, 194847, 185038]\n",
        "\n",
        "# Temporary variables for development:\n",
        "dev_subjects = [100206, 100408]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GCA configurations\n",
        "gca_config = {\n",
        "    \"max_lag\": 1,\n",
        "    \"significance_level\": 0.05,\n",
        "    \"pairwise\": True\n",
        "}\n",
        "\n",
        "#HMM configurations\n",
        "hmm_config = {\n",
        "    \"n_states\": 3,\n",
        "    \"n_iter\": 100,\n",
        "    \"covariance_type\": \"full\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "MdgexxbClVKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3X0Uv3awT3Z"
      },
      "source": [
        "### Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miyVrK7fwVwe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f4cddef4-3aeb-492f-d69f-233aa334acdd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-410051263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m## Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"z_score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \"\"\"\n\u001b[1;32m    112\u001b[0m     \u001b[0mNormalizes\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mfMRI\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mseries\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "# File Access:\n",
        "def generate_paths(task : str = target_tasks[0],\n",
        "                   run  : str = run_direction[0],\n",
        "                   sub_list : list = dev_subjects):\n",
        "    \"\"\"\n",
        "    Uses the paths configured in WoMAD_config to create subject-specific paths.\n",
        "\n",
        "    Arguments:\n",
        "        task             (str): The target task from HCP -> [W]orking [M]emory as default\n",
        "        run              (str): RL or LR -> RL as our arbitrary default\n",
        "        sub_list (list of int): List of target subject ID's as defined in the config file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with this integer and tuple-of-strings format:\n",
        "        paths = {\n",
        "            subject_id (int) : (\"Main 'Results' file path\", \"EV file path\")\n",
        "        }\n",
        "    \"\"\"\n",
        "    paths = {}\n",
        "\n",
        "    # General path format for each subject's directory:\n",
        "    # (f\"../data/HCP_zipped/{subject-ID}/MNINonLinear/Results/\")\n",
        "\n",
        "    # General path format for subjects' task EV files:\n",
        "    # (f\"../data/HCP_zipped/{subject-ID}/MNINonLinear/Results/tfMRI_{TASK}_{RUN}/EVs/\")\n",
        "\n",
        "    # List of target subjects: full_3T_task_subjects (imported from WoMAD_config)\n",
        "    for subject in sub_list:\n",
        "        subject_path    = f\"../data/HCP_zipped/{subject}/MNINonLinear/Results/\"\n",
        "        subject_ev_path = f\"../data/HCP_zipped/{subject}/MNINonLinear/Results/tfMRI_{task}_{run}/EVs/\"\n",
        "        paths[subject]  = (subject_path, subject_ev_path)\n",
        "\n",
        "    return paths\n",
        "\n",
        "\n",
        "def load_data_from_path(task : str = target_tasks[0],\n",
        "                        run  : str = run_direction[0],\n",
        "                        subject : str = dev_subjects[0],\n",
        "                        subtask : str = target_subtasks[\"WM\"][0]):\n",
        "    \"\"\"\n",
        "    Reads the contents of each subject's files.\n",
        "\n",
        "    Arguments:\n",
        "        task    (str): The target task from HCP -> [W]orking [M]emory as default\n",
        "        run     (str): RL or LR -> RL as our arbitrary default\n",
        "        subject (int): ID of specific target subject\n",
        "        subtask (str): The target subtask in string format -> Example: \"0bk_tools\"\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of {Subject: (Tuple of fMRI data)} and\n",
        "        EV file contents assigned to the ev_file variable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        paths = generate_paths(task = task, run = run)\n",
        "        bold_ts_path = paths[subject][0] + f\"tfMRI_{task}_{run}/tfMRI_{task}_{run}_Atlas_MSMAll_hp0_clean_rclean_tclean.dtseries.nii\"\n",
        "        ev_file_path = paths[subject][1] + f\"{subtask}.txt\"\n",
        "\n",
        "        with open(ev_file_path, \"r\") as ev:\n",
        "            ev_file = ev.read()\n",
        "\n",
        "        bold_ts = nib.load(bold_ts_path)\n",
        "        bold_data = bold_ts.get_fdata()\n",
        "        bold_header = bold_ts.header\n",
        "\n",
        "        fmri_timeseries = {subject: (bold_ts, bold_header, bold_data)}\n",
        "\n",
        "        return fmri_timeseries, ev_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error loading time series and EV files from path!\")\n",
        "\n",
        "\n",
        "# Preprocessing:\n",
        "## Parse and Isolate Trials\n",
        "def isolate_trials(fmri_ts, ev_file, TR : float = 0.72):\n",
        "    \"\"\"\n",
        "    Parses through the data and isolates each task trial using EV files.\n",
        "\n",
        "    Input: The fMRI dictionary and EV file from load_data_from_path() function.\n",
        "\n",
        "    Returns:\n",
        "        List of trials isolated using the ev_file.\n",
        "    \"\"\"\n",
        "    trial_list = []\n",
        "\n",
        "    for subject, (bold_ts, bold_header, bold_data) in fmri_ts.items():\n",
        "        data_array = bold_data\n",
        "\n",
        "        try:\n",
        "            ev_data = np.loadtxt(io.StringIO(ev_file))\n",
        "        except ValueError:\n",
        "            print(f\"Could not parse EV file for subject {subject}.\")\n",
        "            continue\n",
        "\n",
        "        for onset, duration, _ in ev_data:\n",
        "            start_idx = int(np.floor(onset / TR))\n",
        "            end_idx = int(np.ceil((onset + duration) / TR))\n",
        "            trial_data = data_array[:, start_idx:end_idx]\n",
        "\n",
        "            trial_list.append({\n",
        "                \"subject\" : subject,\n",
        "                \"onset\" : onset,\n",
        "                \"duration\" : duration,\n",
        "                \"data\" : trial_data\n",
        "            })\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "## Normalization\n",
        "def normalize_data(data : np.ndarray, norm_mode: str = \"z_score\"):\n",
        "    \"\"\"\n",
        "    Normalizes a numpy array of fMRI time series data.\n",
        "\n",
        "    Arguments:\n",
        "        data (np.ndarray): The time series data with shape (voxels, time_points)\n",
        "        norm_mode   (str): Method of normalization (Z score, min/max, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Numpy array of normalized data.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "\n",
        "    if norm_mode == \"z_score\":\n",
        "        ts_data_mean = np.mean(data, axis = 1, keepdims = True)\n",
        "        ts_data_stdv = np.std(data , axis = 1, keepdims = True)\n",
        "\n",
        "        ts_data_stdv[ts_data_stdv == 0] = 1.0\n",
        "\n",
        "        normalized_ts_data = (data - ts_data_mean) / ts_data_stdv\n",
        "\n",
        "        return normalized_ts_data\n",
        "\n",
        "    elif norm_mode == \"min_max\":\n",
        "        min_ts_data = np.min(data, axis = 1, keepdims = True)\n",
        "        max_ts_data = np.max(data, axis = 1, keepdims = True)\n",
        "\n",
        "        range_ts_data = max_ts_data - min_ts_data\n",
        "        range_ts_data[range_ts_data == 0] = 1.0\n",
        "\n",
        "        normalized_ts_data = (data - min_ts_data) / range_ts_data\n",
        "\n",
        "        return normalized_ts_data\n",
        "\n",
        "    else: # For now ...\n",
        "        print(f\"Normalization mode '{norm_mode}' not defined.\\nReturning data as is.\")\n",
        "        return data\n",
        "\n",
        "## Save to Pandas DataFrame\n",
        "def save_to_df(trial_list : List[Dict[str, Any]],\n",
        "               file_name : str,\n",
        "               output_dir : str = processed_path):\n",
        "    \"\"\"\n",
        "    Converts the list of isolated trials to a Pandas DF and saves it to defined path.\n",
        "\n",
        "    Arguments:\n",
        "        trial_list (list): List of {\"subject\", \"onset\", \"duration\", \"data\"} dictionaries.\n",
        "        file_name   (str): Name of output file.\n",
        "        output_dir  (str): Directory for saving the output file.\n",
        "\n",
        "    Saves the pd.DataFrame to output_dir.\n",
        "    \"\"\"\n",
        "    df_from_trial_ts = pd.DataFrame(trial_list)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok = True)\n",
        "    save_path = os.path.join(output_dir, f\"{file_name}.pkl\")\n",
        "\n",
        "    df_from_trial_ts.to_pickle(save_path)\n",
        "\n",
        "    print(f\"Data saved to {save_path}\")\n",
        "\n",
        "    return df_from_trial_ts\n",
        "\n",
        "\n",
        "# Initial Processing (with the WoMAD_data class):\n",
        "class WoMAD_data(Dataset):\n",
        "    def __init__(self,\n",
        "                 task : str,\n",
        "                 runs : list = run_direction,\n",
        "                 subjects : list = dev_subjects,\n",
        "                 output_dir : str = processed_path):\n",
        "        \"\"\"\n",
        "        Basic configuration of the dataset.\n",
        "\n",
        "        Arguments:\n",
        "            task (str): The target task (\"WM\")\n",
        "            runs (list): Run directions\n",
        "            subjects (list): List of target subject IDs\n",
        "            output_dir (str): Directory for saving processed data\n",
        "        \"\"\"\n",
        "        self.task = task\n",
        "        self.runs = runs\n",
        "        self.subjects = subjects\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        self.data = []\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of subjects in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data_paths)\n",
        "\n",
        "    def __getitem__(self, indx: int):\n",
        "        \"\"\"\n",
        "        Loads one trial's data and returns:\n",
        "            - input timeseries (X)\n",
        "            - overall target (Y_overall)\n",
        "            - node target (Y_node)\n",
        "        \"\"\"\n",
        "        trial = self.data[indx]\n",
        "\n",
        "        # Input data with shape (target_nodes, timepoints)\n",
        "        data_np = trial[\"data\"]\n",
        "        data_tensor = torch.from_numpy(data_np).float()\n",
        "\n",
        "        # Overall target:\n",
        "        # TODO: Add overall target components.\n",
        "\n",
        "        # Placeholder:\n",
        "        overall_target_np = trial[\"stats\"][\"trial_mean\"]\n",
        "        overall_target_tensor = torch.tensor([overall_target_np]).float()\n",
        "\n",
        "        # Node target with shape (target_nodes)\n",
        "        node_target_np = trial[\"stats\"][\"mean_per_node\"]\n",
        "        node_target_tensor = torch.from_numpy(node_target_np).float()\n",
        "\n",
        "        return data_tensor, overall_target_tensor, node_target_tensor\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"\n",
        "        Load and parse the data using NIfTI and EV files.\n",
        "        \"\"\"\n",
        "        parsed_data = []\n",
        "\n",
        "        for subject in self.subject:\n",
        "            for run in self.runs:\n",
        "                paths = generate_paths(task = self.task, run = run,\n",
        "                                       sub_list = [subject])\n",
        "\n",
        "                subtasks = target_subtasks.get(self.task, [])\n",
        "\n",
        "                for subtask in subtasks:\n",
        "                    fmri_ts, ev_file = load_data_from_path(task = self.task,\n",
        "                                                           run = run,\n",
        "                                                           subject = subject,\n",
        "                                                           subtask = subtask)\n",
        "                    trial_list_subtask = isolate_trials(fmri_ts, ev_file)\n",
        "\n",
        "                    for trial_dict in trial_list_subtask:\n",
        "                        trial_dict[\"run\"] = run\n",
        "                        trial_dict[\"subtask\"] = subtask\n",
        "                        parsed_data.append(trial_dict)\n",
        "\n",
        "        # TODO: Error handling inside the for loop.\n",
        "\n",
        "        self.data = parsed_data\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def basic_processing(self, norm_mode : str = \"z_score\",\n",
        "                         file_name : str = \"processed_fMRI_data\"):\n",
        "        \"\"\"\n",
        "        Normalization and saving with normalize_data() and save_to_df().\n",
        "        \"\"\"\n",
        "        processed_trials = []\n",
        "\n",
        "        for trial in self.data:\n",
        "            normalized_trial = normalize_data(trial[\"data\"], norm_mode = norm_mode)\n",
        "\n",
        "            trial[\"data\"] = normalized_trial\n",
        "            trial[\"norm_mode\"] = norm_mode\n",
        "            processed_trials.append(trial)\n",
        "\n",
        "        file_to_save_processed_data = f\"{file_name}_{self.task}_{norm_mode}\"\n",
        "        self.processed_df = save_to_df(processed_trials,\n",
        "                                       file_to_save_processed_data,\n",
        "                                       self.output_dir)\n",
        "\n",
        "        self.data = processed_trials\n",
        "\n",
        "        return self.processed_df\n",
        "\n",
        "    def _calc_corr_matrix(self, trial_data: np.ndarray) -> dict:\n",
        "        \"\"\"\n",
        "        Calculates whole-brain and network-level correlation matrices for a single isolated trial.\n",
        "\n",
        "        NOTE:   Network-level correlations require network masks\n",
        "                or specific voxel/parcel definitions.\n",
        "                These have not yet been defined in the config files.\n",
        "\n",
        "        Arguments:\n",
        "            trial_data (np.ndarray): The isolated trial's time series data.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the calculated correlation matrix.\n",
        "        \"\"\"\n",
        "        # All voxels (whole_brain)\n",
        "        whole_brain_corr_mat = np.corrcoef(trial_data)\n",
        "\n",
        "        whole_brain_corr_mat[np.isnan(whole_brain_corr_mat)] = 0\n",
        "\n",
        "        # TODO: Create network-level correlation\n",
        "\n",
        "        return {\n",
        "            \"whole_brain\"  : whole_brain_corr_mat,\n",
        "            \"network_level\": whole_brain_corr_mat   # Temp solution until the network-level is defined.\n",
        "        }\n",
        "\n",
        "    def calc_func_connectivity(self):\n",
        "        \"\"\"\n",
        "        Calculates the correlation matrices for all isolated trials.\n",
        "\n",
        "        Returns:\n",
        "            The updated list of trial dictionaries with the correlation matrices added.\n",
        "        \"\"\"\n",
        "        for trial in self.data:\n",
        "            trial_ts_data = trial.get(\"data\") # Should be normalized\n",
        "\n",
        "            # Correlation matrix\n",
        "            trial_corr_mat = self._calc_corr_matrix(trial_ts_data)\n",
        "\n",
        "            trial[\"corr_matrix\"] = trial_corr_mat\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def calc_basic_stats(self):\n",
        "        \"\"\"\n",
        "        Calculate basic statistics (mean and std) of\n",
        "        the activity for isolated trials across all nodes/voxels.\n",
        "\n",
        "        Returns:\n",
        "            Updated list of trial dictionaries with \"stats\" key added.\n",
        "        \"\"\"\n",
        "        for trial in self.data:\n",
        "            trial_ts_data = trial.get(\"data\")\n",
        "\n",
        "            if trial_ts_data is None:\n",
        "                continue\n",
        "\n",
        "            # Mean activity\n",
        "            node_mean_activity = np.mean(trial_ts_data, axis = 1)\n",
        "\n",
        "            # Standard deviation\n",
        "            node_std_activity = np.std(trial_ts_data, axis = 1)\n",
        "\n",
        "            # Overall average for entire trial\n",
        "            trial_mean_activity = np.mean(node_mean_activity)\n",
        "            trial_mean_of_std   = np.mean(node_std_activity)\n",
        "\n",
        "            trial[\"stats\"] = {\n",
        "                \"mean_per_node\": node_mean_activity,\n",
        "                \"std_per_node\" : node_std_activity,\n",
        "                \"trial_mean\"   : trial_mean_activity,\n",
        "                \"overall_std\"  : trial_mean_of_std\n",
        "            }\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def visualize_correlations(self,\n",
        "                               target_trial_idx: int = 0,\n",
        "                               matrix_type: str = \"whole_brain\"):\n",
        "        \"\"\"\n",
        "        Generate a heatmap visualization for one trial's correlation matrix.\n",
        "\n",
        "        Arguments:\n",
        "            target_trial_idx (int): Index of the trial in self.data\n",
        "            matrix_type      (str): The type of matrix to plot\n",
        "                                    \"whole_brain\" or \"network_level\"\n",
        "        \"\"\"\n",
        "        trial = self.data[target_trial_idx]\n",
        "        corr_matrices = trial.get(\"corr_matrix\")\n",
        "\n",
        "        matrix_to_plot = corr_matrices.get(matrix_type)\n",
        "\n",
        "        plt.figure(figsize = (10, 8))\n",
        "        sns.heatmap(matrix_to_plot,\n",
        "                    cmap = \"RdBu_r\",\n",
        "                    vmin = -1, vmax = 1,\n",
        "                    square = True,\n",
        "                    cbar_kws = {\"label\" : \"Pearson Correlation Coefficient\"})\n",
        "\n",
        "        subject_id = trial.get(\"subject\", \"N/A\")\n",
        "\n",
        "        task = trial.get(\"task\", \"N/A\")\n",
        "        subtask = trial.get(\"subtask\", \"N/A\")\n",
        "\n",
        "        plt.title(f\"{matrix_type.title()} Correlation Matrix\\nSubject: {subject_id}, {task}: {subtask}, Trial {target_trial_idx}\")\n",
        "        plt.xlabel(\"Node/Voxel Index\")\n",
        "        plt.ylabel(\"Node/Voxel Index\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# TODO: Add function for \"validation set processing\" which can process non-HCP data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrYSUIinxCXs"
      },
      "source": [
        "### Model Setup Module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Dynamic Input module"
      ],
      "metadata": {
        "id": "3Tj7naFdXzP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamic Adapter\n",
        "TARGET_NODE_COUNT = 360       # 360 parcels based on HCP and Glasser parcellation\n",
        "\n",
        "class DynamicInput(nn.Module):\n",
        "    \"\"\"\n",
        "    This module handles input data with different number of voxels\n",
        "    and adapts it for the modules (Info flow or Core) of the WoMAD model.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_nodes: int = TARGET_NODE_COUNT):\n",
        "        super().__init__()\n",
        "        self.target_nodes = target_nodes\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Dynamically adapts the input data to defined dimension.\n",
        "\n",
        "        Input:\n",
        "            x : 3D tensor with shape (batch, current_nodes, timepoints)\n",
        "\n",
        "        Output:\n",
        "            The dynamically adapted 3D tensor with shape (batch, target_nodes, timepoints)\n",
        "        \"\"\"\n",
        "        batch, current_nodes, timepoints = x.shape\n",
        "\n",
        "        if current_nodes == self.target_nodes:\n",
        "            return x\n",
        "\n",
        "        elif current_nodes > self.target_nodes:\n",
        "            # TODO: Add voxel-to-parcel downsampling (requires a parcellation map)\n",
        "\n",
        "            # Current solution for downsampling: Adaptive Pooling (linear projection)\n",
        "            x_reshaped = x.transpose(1, 2)\n",
        "\n",
        "            x_pooled = F.adaptive_avg_pool1d(x_reshaped, self.target_nodes)\n",
        "\n",
        "            x_pooled_out = x_pooled.transpose(1, 2)\n",
        "\n",
        "            return x_pooled_out\n",
        "\n",
        "        else: # current_nodes < self.target_nodes\n",
        "            required_padding = self.target_nodes - current_nodes\n",
        "\n",
        "            x_padded = F.pad(x, (0, 0, 0, required_padding))\n",
        "\n",
        "            return x_padded"
      ],
      "metadata": {
        "id": "6UmT9Fq1XxwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kTxQZ2r0Tlj"
      },
      "source": [
        "#### Info flow submodule\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0BQTRbo0TOf"
      },
      "outputs": [],
      "source": [
        "#GCA\n",
        "from scipy import stats\n",
        "from typing import Tuple, Dict, Optional\n",
        "import warnings\n",
        "\n",
        "\n",
        "class GrangerCausalityAnalysis:\n",
        "    def __init__(self, config: dict):\n",
        "        \"\"\"\n",
        "        Sets up the complete WoMAD model with all modules and submodules.\n",
        "        (Each module and submodule includes a dynamic input layer that matches the size of input.)\n",
        "        \"\"\"\n",
        "        # Information Flow Module\n",
        "        ## Effective Connectivity: GCA\n",
        "        ## Dynamic Functional Connectivity: HMM\n",
        "        ## Final info-flow manifold: Temporal GNN\n",
        "\n",
        "    def __init__(self, config:dict):\n",
        "        super(WoMAD_info_flow, self).__init__()\n",
        "        self.target_nodes = config.get(\"target_nodes\", 360)\n",
        "        self.lstm_hidden = config[\"lstm_config\"][\"hidden_size\"]\n",
        "\n",
        "        # GCA Layer: A simple linear layer to learn directed weights between nodes\n",
        "        self.gca_weights = nn.Linear(self.target_nodes, self.target_nodes, bias=False)\n",
        "\n",
        "        # Temporal GNN components\n",
        "        # Placeholder for torch_geometric layers\n",
        "        # self.gnn_layer = GATConv(in_channels=..., out_channels=...)\n",
        "\n",
        "    def _calculate_gca(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Estimates directed connectivity.\n",
        "        Input shape: (batch, nodes, timepoints) [cite: 385]\n",
        "        \"\"\"\n",
        "        # Shift time to compare past (t) with future (t+1)\n",
        "        past = x[:, :, :-1]   # All timepoints except last\n",
        "        future = x[:, :, 1:]  # All timepoints except first\n",
        "\n",
        "        # Linear regression to find influence weights\n",
        "        # future = weights * past\n",
        "        # This simplifies the VAR(1) model for neural network integration\n",
        "        directed_adj = self.gca_weights(past.transpose(1, 2))\n",
        "        return directed_adj.transpose(1, 2)\n",
        "\n",
        "    def _get_hmm_states(self, trial_data, n_states=3):\n",
        "        \"\"\"\n",
        "        Identifies cognitive states (Dynamic Functional Connectivity).\n",
        "        Note: HMM is often pre-computed or run via CPU as it is non-differentiable.\n",
        "        \"\"\"\n",
        "        # trial_data shape: (nodes, time)\n",
        "        X = trial_data.cpu().detach().numpy().T\n",
        "        model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=100)\n",
        "        model.fit(X)\n",
        "        state_sequence = model.predict(X)\n",
        "        return state_sequence\n",
        "\n",
        "    def forward(self, input: torch.Tensor, module_selection: str):\n",
        "        \"\"\"\n",
        "        The forward pass that manages how the data passes through modules.\n",
        "        \"\"\"\n",
        "        if module_selection == \"gca\":\n",
        "          return self._calculate_gca(input)\n",
        "\n",
        "        return input\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HMM\n",
        "!pip install --quiet hmmlearn\n",
        "from hmmlearn import hmm\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def run_hmm_stating(trial_data, np.ndarray,\n",
        "                    n_states: int = 3,\n",
        "                    n_iter: int = 100,\n",
        "                    n_pca_components: int = None) --> Dict:\n",
        "    \"\"\"\n",
        "    Identifies hidden brain states using Hidden Markov Model.\n",
        "\n",
        "    Args:\n",
        "        trial_data (np.ndarray): fMRI data (n_nodes, n_timepoints).\n",
        "        n_states: Number of hidden states to detect.\n",
        "        n_iter: Number of EM iterations.\n",
        "        n_pca_components: Number of principal components to use for PCA.\n",
        "\n",
        "    Returns:\n",
        "        Dict with state_sequence, state_means, transition_matrix\n",
        "    \"\"\"\n",
        "\n",
        "    #HMM expects (n_samples, n_features) = (timepoints, nodes)\n",
        "    X = trial_data.T\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    # Use PCA if features > samples to avoid degenerate solution\n",
        "    pca_used = False\n",
        "    if n_pca_components is None:\n",
        "        # Keep n_features such that model parameters < data points\n",
        "        max_components = min(n_samples - 1, n_features, 20)\n",
        "        if n_features > max_components:\n",
        "            n_pca_components = max_components\n",
        "\n",
        "    if n_pca_components and n_pca_components < n_features:\n",
        "        pca = PCA(n_components=n_pca_components, random_state=42)\n",
        "        X = pca.fit_transform(X)\n",
        "        pca_used = True\n",
        "\n",
        "    # Fit HMM with diagonal covariance for efficiency\n",
        "    model = hmm.GaussianHMM(\n",
        "        n_components=n_states,\n",
        "        covariance_type=\"diag\",\n",
        "        n_iter=n_iter,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=100)\n",
        "\n",
        "    model.fit(X)\n",
        "\n",
        "    state_sequence = model.predict(X)\n",
        "\n",
        "    return {\n",
        "        'state_sequence': state_sequence,\n",
        "        'state_means': model.means_,\n",
        "        'transition_matrix': model.transmat_,\n",
        "        'n_states': n_states,\n",
        "        'pca_used': pca_used,\n",
        "        'n_features_used': X.shape[1]\n",
        "    }\n",
        "\n",
        "def compute_state_connectivity(trial_data: np.ndarray,\n",
        "                               state_sequence: np.ndarray) -> Dict[int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute functional connectivity matrix for each HMM state.\n",
        "\n",
        "    Args:\n",
        "        trial_data: fMRI data (n_nodes, n_timepoints)\n",
        "        state_sequence: HMM state labels for each timepoint\n",
        "\n",
        "    Returns:\n",
        "        Dict mapping state_id -> correlation matrix\n",
        "    \"\"\"\n",
        "    unique_states = np.unique(state_sequence)\n",
        "    state_connectivity = {}\n",
        "\n",
        "    for state in unique_states:\n",
        "        state_mask = state_sequence == state\n",
        "        if np.sum(state_mask) < 2:\n",
        "            continue\n",
        "\n",
        "        # Extract timepoints for this state\n",
        "        state_data = trial_data[:, state_mask]\n",
        "\n",
        "        # Compute correlation matrix\n",
        "        corr_matrix = np.corrcoef(state_data)\n",
        "        corr_matrix[np.isnan(corr_matrix)] = 0\n",
        "\n",
        "        state_connectivity[int(state)] = corr_matrix\n",
        "\n",
        "    return state_connectivity"
      ],
      "metadata": {
        "id": "AckY1XRgiwJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Topological Similarity & Graph Overlap Analysis"
      ],
      "metadata": {
        "id": "9ywnHHFon1Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Temporal GNN & Info Flow Integration\n"
      ],
      "metadata": {
        "id": "3_hG3SdrlCQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class WoMAD_info_flow(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        super(WoMAD_info_flow, self).__init__()\n",
        "        self.target_nodes = 360 # Defined in your config [cite: 374, 442]\n",
        "        self.hidden_size = 128 # Defined in your LSTM config [cite: 61, 439]\n",
        "\n",
        "        # The GNN Layer: Learns directed attention between parcels\n",
        "        # in_channels: BOLD activity at each node\n",
        "        # out_channels: The hidden representation of \"info flow\"\n",
        "        self.gnn_manifold = GATConv(in_channels=1,\n",
        "                                    out_channels=self.hidden_size,\n",
        "                                    heads=4,\n",
        "                                    concat=True)\n",
        "\n",
        "        # Linear layer to condense multi-head attention back to hidden_size\n",
        "        self.post_gnn = nn.Linear(self.hidden_size * 4, self.hidden_size)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        x: BOLD signal (batch, nodes, 1)\n",
        "        edge_index: The directed connections from GCA (2, num_edges)\n",
        "        edge_attr: The strength of those connections (num_edges, 1)\n",
        "        \"\"\"\n",
        "        # Step 3: Message Passing\n",
        "        # This passes information along the GCA-defined directed paths\n",
        "        out = self.gnn_manifold(x, edge_index, edge_attr)\n",
        "        out = F.elu(self.post_gnn(out))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-pP75Cnst5Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGBI_8EA0T35"
      },
      "source": [
        "#### Core submodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW-wsS1ixO-F"
      },
      "outputs": [],
      "source": [
        "# Core module\n",
        "class WoMAD_core(nn.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        \"\"\"\n",
        "        Sets up the complete WoMAD model with all modules and submodules.\n",
        "        (Each module and submodule includes a dynamic input layer that matches the size of input.)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        target_nodes = WoMAD_config.target_parcellation         # 360, Temporary.\n",
        "        timepoints = WoMAD_config.target_timepoints             # 20, Temporary.\n",
        "\n",
        "        lstm_h_size = WoMAD_config.lstm_config[\"hidden_size\"]   # 128\n",
        "\n",
        "        conv4d_out_size = 64                                    # From simplified config\n",
        "\n",
        "        # Dynamic Adapter\n",
        "        self.dyn_input_adapter = DynamicInput(target_nodes = 360)\n",
        "\n",
        "        # Core Module\n",
        "        ## Submodule A: 3D-UNet\n",
        "        ## Input shape = (batch, target_nodes, timepoints)\n",
        "        self.segment_and_label = nn.Sequential(\n",
        "            nn.Conv1d(in_channels = target_nodes,\n",
        "                      out_channels = target_nodes,\n",
        "                      kernel_size = 3, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Identity()          # FIX: Placeholder should be replaced with the 3D-UNet.\n",
        "        )\n",
        "\n",
        "        ## Parallel submodule B-1: LSTM (Temporal features)\n",
        "        self.temporal_lstm = nn.LSTM(input_size  = target_nodes,\n",
        "                                     hidden_size = lstm_h_size,\n",
        "                                     num_layers  = WoMAD_config.lstm_config[\"num_layers\"],\n",
        "                                     dropout     = WoMAD_config.lstm_config[\"dropout\"],\n",
        "                                     batch_first = True)\n",
        "\n",
        "        ## Parallel submodule B-2: ConvNet4D (Spatiotemporal features)\n",
        "        self.spatiotemporal_cnv4d = nn.Sequential(\n",
        "            nn.Conv2d(in_channels  = 1,\n",
        "                      out_channels = 32,\n",
        "                      kernel_size  = (3, 3), padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = (2, 2)),\n",
        "            nn.Conv2d(in_channels  = 32,\n",
        "                      out_channels = conv4d_out_size,\n",
        "                      kernel_size  = (3, 3), padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        ## Submodule C: Fusion Layer\n",
        "        fusion_input_size = lstm_out_size + conv4d_out_size     # 192\n",
        "        self.fusion_block = nn.Sequential(\n",
        "            nn.Linear(fusion_input_size, WoMAD_config.fusion_config[\"hidden_size\"]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        ### Overall, WM-based activity score:\n",
        "        self.overall_activity_score = nn.Linear(WoMAD_config.fusion_config[\"hidden_size\"], 1)\n",
        "\n",
        "        ### Node-based (voxel-based or parcel-based) activity scores:\n",
        "        self.node_wise_activity_scores = nn.Linear(WoMAD_config.fusion_config[\"hidden_size\"], target_nodes)\n",
        "\n",
        "    def _prepare_4d_data(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Helper method to create the 4D network for the second module.\n",
        "\n",
        "        Input:\n",
        "            Tensor with shape (batch, target_nodes, timepoints).\n",
        "            target_nodes is the flat spatial dimension.\n",
        "\n",
        "        Output:\n",
        "            5D tensor for the 4D ConvNet with\n",
        "            shape (batch, C=1, timepoints, X, Y, Z)\n",
        "        \"\"\"\n",
        "        batch, nodes, timepoints = input.shape\n",
        "        # TODO: Create the mapping array to place nodes into a X*Y*Z grid.\n",
        "\n",
        "        four_dim_data = input.unsqueeze(1)\n",
        "\n",
        "        return four_dim_data\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The forward pass that manages how the data passes through modules.\n",
        "\n",
        "        Sequence for forward pass:\n",
        "            Input -> Segmentation (3D-UNet) -> (LSTM, Conv4D) -> Fusion\n",
        "\n",
        "        Input:\n",
        "            Tensor with shape (batch, current_nodes, timepoints)\n",
        "        \"\"\"\n",
        "        # Dynamic Input Adaption\n",
        "        x_dynamically_adapted = self.dyn_input_adapter(input)\n",
        "\n",
        "        # 3D-UNet\n",
        "        unet_out_timeseries = self.segment_and_label(x_dynamically_adapted)\n",
        "\n",
        "        # LSTM\n",
        "        x_for_lstm = unet_out_timeseries.transpose(1, 2)\n",
        "        _, (h_n, _) = self.temporal_lstm(x_for_lstm)\n",
        "        lstm_out = h_n[-1]\n",
        "\n",
        "        # ConvNet4D\n",
        "        x_for_conv4d = self._prepare_4d_data(unet_out_timeseries)\n",
        "        conv4d_out = self.spatiotemporal_cnv4d(x_for_conv4d)\n",
        "\n",
        "        # Fusion Layer\n",
        "        fused_feats = torch.cat([lstm_out, conv4d_out], dim = 1)\n",
        "        shared_features = self.shared_fusion_block(fused_feats)\n",
        "\n",
        "        overall_score = self.overall_activity_score(shared_features)\n",
        "        node_scores   = self.node_wise_activity_scores(shared_features)\n",
        "\n",
        "        return overall_score, node_scores\n",
        "\n",
        "\n",
        "def model_config(config: dict) -> WoMAD_core:\n",
        "    \"\"\"\n",
        "    Initialized WoMAD and moves it to the device.\n",
        "\n",
        "    Argument:\n",
        "        config (dict): WoMAD config dictionary\n",
        "\n",
        "    Returns:\n",
        "        WoMAD: Model ready to be trained.\n",
        "    \"\"\"\n",
        "    model = WoMAD_core(config)\n",
        "\n",
        "    if config[\"system\"][\"use_gpu\"] and torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynUZIOO_xPPK"
      },
      "source": [
        "### Hyperparameter Module - NOT FINALIZED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsM6PYqYxSc7"
      },
      "outputs": [],
      "source": [
        "def define_search_space(trial: optuna.Trial) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Define the search space for hyperparameter optimization.\n",
        "\n",
        "    Arguments:\n",
        "        trial (optuna.Trial): The trial object that suggests parameters\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary of suggested hyperparameters.\n",
        "    \"\"\"\n",
        "    # Main parameters (Learning rate, batch size, number of epochs)\n",
        "    learning_rate = 0\n",
        "    batch_size = 0\n",
        "    epochs = 0\n",
        "    # Model-specific parameters (Hidden layers, dropout rates)\n",
        "    hidden_layers = 0\n",
        "    dropout_rate = 0\n",
        "\n",
        "    suggested_parameters = {\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\"   : batch_size,\n",
        "        \"epochs\"       : epochs,\n",
        "        \"hidden_layers\": hidden_layers,\n",
        "        \"dropout_rate\" : dropout_rate\n",
        "    }\n",
        "\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    \"\"\"\n",
        "    TO DO: Define the objective function for Optuna.\n",
        "    \"\"\"\n",
        "    hyperparameters = define_search_space(trial)\n",
        "\n",
        "    # config = WoMAD_config.load_config()\n",
        "    config[\"training\"].update(hyperparameters)\n",
        "\n",
        "    final_valid_metric = run_pipeline(config)\n",
        "\n",
        "    return final_valid_metric\n",
        "\n",
        "def run_hyperparameter_optim():\n",
        "    \"\"\"\n",
        "    TO DO: Define the main hyperparameter search function.\n",
        "    \"\"\"\n",
        "    # Define target for optimization (min loss, min MSE, etc.)\n",
        "\n",
        "    # Print and save the results (best trial, best parameters, best target metric)\n",
        "\n",
        "    # Save to file as well\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_hyperparameter_optim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKfu-ry5ygV5"
      },
      "source": [
        "### Model Train Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlitOhE5zAiV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "d53402d5-ff5d-4152-9251-638219a7fe7b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "attempted relative import with no known parent package",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4200549334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWoMAD_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_setup_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDynamicInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWoMAD_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from . import WoMAD_config\n",
        "\n",
        "from .model_setup_module import DynamicInput, WoMAD_core\n",
        "from .model_valid_module import run_valid_epoch\n",
        "\n",
        "def WoMAD_optimizer(model: nn.Module, config: dict) -> torch.optim.Optimizer:\n",
        "    \"\"\"\n",
        "    Configures the optimizer for WoMAD model.\n",
        "    \"\"\"\n",
        "    lr = WoMAD_config.training_config[\"learning_rate\"]\n",
        "    return torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "def WoMAD_loss_function(config: dict) -> Dict[str, nn.Module]:\n",
        "    \"\"\"\n",
        "    Create a dictionary of loss functions.\n",
        "    \"\"\"\n",
        "    loss_func_dict = {\n",
        "        \"overall_score_loss\": nn.MSELoss(),\n",
        "        \"node_score_loss\"   : nn.MSELoss()\n",
        "    }\n",
        "    return loss_func_dict\n",
        "\n",
        "def run_training_epoch(model: WoMAD_core, data_loader: DataLoader,\n",
        "                       optimizer: torch.optim.Optimizer,\n",
        "                       loss_funcs: Dict[str, nn.Module],\n",
        "                       epoch: int, config: dict):\n",
        "    \"\"\"\n",
        "    Function to run a single training epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    overall_weight = WoMAD_config.training_loss_weights[\"overall_loss_weight\"]\n",
        "    node_weight    = WoMAD_config.training_loss_weights[\"node_loss_weight\"]\n",
        "\n",
        "    overall_loss_fn = loss_funcs[\"overall_score_loss\"]\n",
        "    node_loss_fn    = loss_funcs[\"node_score_loss\"]\n",
        "\n",
        "    for batch_indx, (data, overall_target, node_target) in enumerate(data_loader):\n",
        "        overall_target = overall_target.float()\n",
        "        node_target    = node_target.float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to return (overall and node-wise prediction)\n",
        "        overall_pred, node_pred = model(data)\n",
        "\n",
        "        # Calculate losses\n",
        "        loss_overall = overall_loss_fn(overall_pred.squeeze(), overall_target)\n",
        "        loss_nodes   = node_loss_fn(node_pred, node_target)\n",
        "\n",
        "        combined_loss = (overall_weight * loss_overall) + (node_weight * loss_nodes)\n",
        "\n",
        "        # Backpropagate\n",
        "        combined_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += combined_loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "    avg_loss = total_train_loss / total_samples\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {avg_loss: .6f}\")\n",
        "    return avg_loss\n",
        "\n",
        "def run_kfold_training(dataset, config: dict):\n",
        "    \"\"\"\n",
        "    Executes K-fold cross validation for training.\n",
        "\n",
        "    Arguments:\n",
        "        dataset (Dataset): The WoMAD data which contains all target subject data.\n",
        "        config     (dict): Configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries with training stats for each training fold.\n",
        "    \"\"\"\n",
        "    k_folds = WoMAD_config.training_config[\"k_folds\"]\n",
        "    num_epochs = WoMAD_config.training_config[\"num_epochs\"]\n",
        "    batch_size = WoMAD_config.training_config[\"batch_size\"]\n",
        "\n",
        "    kfold = KFold(n_splits = k_folds, shuffle = True, random_state = 42)\n",
        "    all_kfold_train_stats = []\n",
        "\n",
        "    loss_funcs = WoMAD_loss_function(config)\n",
        "\n",
        "    print(f\"K-fold cross-validation for {k_folds} folds over {len(dataset)} trials:\")\n",
        "\n",
        "    for fold, (train_indx, valid_indx) in enumerate(kfold.split(dataset)):\n",
        "        print(f\"\\nFold {fold + 1}/{k_folds}:\")\n",
        "\n",
        "        train_subset = Subset(dataset, train_indx)\n",
        "        valid_subset = Subset(dataset, valid_indx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle = True)\n",
        "        valid_loader = DataLoader(valid_subset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "        print(f\"Train samples: {len(train_subset)}, Validation samples: {len(valid_subset)}\")\n",
        "\n",
        "        # Model initiation and setup\n",
        "        model = WoMAD_core(config)\n",
        "        # TODO: Add the device logic (model.cuda())\n",
        "        optimizer = WoMAD_optimizer(model, config)\n",
        "\n",
        "        fold_history = {\"train_loss\"  : [],\n",
        "                        \"valid_loss\"  : [],\n",
        "                        \"val_metrics\" : []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = run_training_epoch(model, train_loader, optimizer, loss_funcs, epoch, config)\n",
        "            fold_history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "            valid_loss, val_metrics = run_valid_epoch(model, val_loader, loss_funcs, epoch, config)\n",
        "\n",
        "            fold_history[\"valid_loss\"].append(valid_loss)\n",
        "            fold_history[\"val_metrics\"].append(val_metrics)\n",
        "\n",
        "        all_kfold_train_stats.append({\"fold\": fold + 1, \"history\": fold_history})\n",
        "\n",
        "        print(\"\\nK-fold training complete.\")\n",
        "\n",
        "    return all_kfold_train_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqAcw_RUymGR"
      },
      "source": [
        "### Model Valid Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf1WUvxdzAFP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import Dict, Tuple, Any\n",
        "\n",
        "from . import WoMAD_config\n",
        "\n",
        "def calc_graph_overlap():\n",
        "    \"\"\"\n",
        "    TO DO: Create function to calculate graph overlap for Info-Flow module.\n",
        "    \"\"\"\n",
        "    # Create graph network with the info-flow output.\n",
        "    # Calculate graph overlap:\n",
        "    ## Step 1: Normalization (Nodes and weights)\n",
        "    ## Step 2: Calculate overlap (Edge-wise percentage with thresholding and Jaccard Indx\n",
        "    ##         and Weighted correlation with vectorization and Pearson's\n",
        "    ## Step 3: Analyze topological similarity (Compare architecture using key topological metrics)\n",
        "\n",
        "def calc_dice_coeff():\n",
        "    \"\"\"\n",
        "    TO DO: Create function to calculate Dice coefficient.\n",
        "    \"\"\"\n",
        "    dice_coeff = 0\n",
        "\n",
        "    return dice_coeff\n",
        "\n",
        "def calc_r_sqrd():\n",
        "    \"\"\"\n",
        "    TO DO: Create function to calculate R^2 score.\n",
        "    \"\"\"\n",
        "    r_sqrd = 0\n",
        "\n",
        "    return r_sqrd\n",
        "\n",
        "def calc_all_metrics():         # NOTE: This is for the information flow module.\n",
        "    \"\"\"\n",
        "    TO DO: Create the function to calculate all metrics (Dice, MSE, R^2, overall score)\n",
        "    \"\"\"\n",
        "    dice_score = calc_dice_coeff()\n",
        "    mean_sqrd_err = calc_mse()\n",
        "    r_sqrd = calc_r_sqrd()\n",
        "\n",
        "    overall_score = (dice_score + r_sqrd) / 2\n",
        "\n",
        "    metrics_dict = {\n",
        "        \"Dice_coefficient\": dice_score,\n",
        "        \"Mean_Sqrd_Error\" : mean_sqrd_err,\n",
        "        \"R_squared\"       : r_sqrd,\n",
        "        \"Overall_score\"   : overall_score\n",
        "    }\n",
        "\n",
        "    return metrics_dict\n",
        "\n",
        "def run_valid_epoch(model, data_loader: DataLoader,\n",
        "                    loss_funcs: Dict[str, nn.Module],\n",
        "                    epoch: int, config: dict\n",
        "                    ) -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Runs one validation epochs and calculates loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_overall_pred = []\n",
        "    all_overall_target = []\n",
        "    all_node_pred = []\n",
        "    all_node_target = []\n",
        "\n",
        "    overall_weight = WoMAD_config.training_loss_weights[\"overall_loss_weight\"]\n",
        "    node_weight    = WoMAD_config.training_loss_weights[\"node_loss_weight\"]\n",
        "\n",
        "    overall_loss_fn = loss_funcs[\"overall_score_loss\"]\n",
        "    node_loss_fn    = loss_funcs[\"node_score_loss\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, overall_target, node_target in data_loader:\n",
        "            overall_target = overall_target.float()\n",
        "            node_target = node_target.float()\n",
        "\n",
        "            overall_pred, node_pred = model(data)\n",
        "\n",
        "            loss_overall = overall_loss_fn(overall_pred.squeeze(), overall_target)\n",
        "            loss_node    = node_loss_fn(node_pred, node_target)\n",
        "            combined_loss = (overall_weight  * loss_overall) + (node_weight * loss_node)\n",
        "\n",
        "            total_val_loss += combined_loss.item() * data.size(0)\n",
        "            total_samples  += data.size(0)\n",
        "\n",
        "            all_overall_pred.append(overall_pred)\n",
        "            all_node_pred.append(node_pred)\n",
        "\n",
        "            all_overall_target.append(overall_target)\n",
        "            all_node_target.append(node_target)\n",
        "\n",
        "    avg_loss = total_val_loss / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Validation Loss: {avg_loss: .6f}\")\n",
        "\n",
        "    final_overall_pred   = torch.cat(all_overall_pred).squeeze()\n",
        "    final_overall_target = torch.cat(all_overall_target)\n",
        "\n",
        "    final_node_pred   = torch.cat(all_node_pred)\n",
        "    final_node_target = torch.cat(all_node_target)\n",
        "\n",
        "    metrics = calc_all_metrics(final_overall_pred,\n",
        "                               final_node_pred,\n",
        "                               final_overall_target,\n",
        "                               final_node_target)\n",
        "\n",
        "    print(f\"Validation metrics: {metrics}\")\n",
        "\n",
        "    return avg_loss, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph62Y1AvyqnY"
      },
      "source": [
        "### Result Interpretation Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5asSLCsRyvAu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "import torch\n",
        "from typing import Dict, Any\n",
        "\n",
        "from . import WoMAD_config\n",
        "\n",
        "def predict():\n",
        "    \"\"\"\n",
        "    TO DO: Create the functions that allows us to use the model for inference.\n",
        "    \"\"\"\n",
        "    return prediction\n",
        "\n",
        "def visualize_and_interpret(model: WoMAD_core,\n",
        "                            data_loader: DataLoader,\n",
        "                            config: dict):\n",
        "    \"\"\"\n",
        "    Generating figures and saliency maps for interpreting the results.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Visualize output (predicted vs. actual score)\n",
        "\n",
        "    # SHAP analysis based on timeseries and final fused outputs\n",
        "\n",
        "    # Save visuals\n",
        "    return 0\n",
        "\n",
        "def run_pipeline_with_valid_dataset(model: WoMAD_core,\n",
        "                                    valid_loader: DataLoader,\n",
        "                                    loss_funcs: Dict[str, nn.Module],\n",
        "                                    config: dict):\n",
        "    \"\"\"\n",
        "    Running full validation and analysis after training.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Validation loop\n",
        "    all_preds, all_targets = which_function_is_this(model, valid_loader, loss_funcs, config)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = calc_all_metrics(all_preds[\"overall\"],\n",
        "                               all_preds[\"node\"],\n",
        "                               all_targets[\"overall\"],\n",
        "                               all_targets[\"node\"])\n",
        "    print(f\"Final validation metrics: {metrics}\")\n",
        "\n",
        "    # Visuals\n",
        "    visualize_and_interpret(model, valid_loader, config)\n",
        "\n",
        "    print(\"Validation complete.\")\n",
        "\n",
        "    # Save final metrics\n",
        "    # print(f\"Outputs saved to: {output_dir}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-G7lwBhzdyg"
      },
      "source": [
        "## TESTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub46Da7Izfx_"
      },
      "outputs": [],
      "source": [
        "# TODO: Create dummy data OR sample a tiny subset of the actual dataset\n",
        "# TODO: Create tests for each and every single function or method.\n",
        "\n",
        "# TESTS:\n",
        "def test_data_module():\n",
        "\n",
        "def test_model_setup():\n",
        "\n",
        "def test_model_training():\n",
        "\n",
        "def test_model_validation():\n",
        "\n",
        "def test_result_interpretation():"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SozhrodLzawR"
      },
      "source": [
        "## WoMAD Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1W_Ab2uz3hk"
      },
      "outputs": [],
      "source": [
        "# Terminal functions and UI\n",
        "## print status, success, or error\n",
        "## clear screen\n",
        "## Welcome/Completion\n",
        "\n",
        "def run_WoMAD(config):\n",
        "    # Environment setup\n",
        "    # Data and initial processing\n",
        "    # Model setup\n",
        "    # Training (and hyperparameter search)\n",
        "    # Post-training: Analysis, Visualization, and Interpretation\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # config = WoMAD_config.load_config()\n",
        "    run_WoMAD(config)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}