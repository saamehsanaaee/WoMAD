{"cells":[{"cell_type":"markdown","metadata":{"id":"qlvT2KwSvKHe"},"source":["# WoMAD Development Notebook"]},{"cell_type":"markdown","metadata":{"id":"cIyqEvZkvTQ5"},"source":["### Setup and Configurations"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ByjkqCGNvdMc","outputId":"046e0e64-e9f9-4cf0-b3cd-23e7f71bc31d","executionInfo":{"status":"ok","timestamp":1770147454794,"user_tz":300,"elapsed":4017521,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_spline_conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install basic dependencies\n","!pip install --quiet requests nilearn nibabel brainspace numpy pandas scikit-learn torch torch-geometric scipy rich>=13.5.2\n","!pip install --quiet optuna\n","\n","# Install torch_geometric and its dependencies\n","!pip install --quiet torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n","\n","import os\n","import io\n","import time\n","import zipfile\n","from typing import List, Dict, Tuple, Any, Callable\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch_geometric.data as PyG_Data\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","import pandas as pd\n","import nibabel as nib\n","\n","import shap   # For result interp module\n","import pytest # For TESTS module\n","import optuna # For hyperparameter module"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18728,"status":"ok","timestamp":1770150315095,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"},"user_tz":300},"id":"y0f-lhxivPtZ","outputId":"39f59bd2-829c-40c4-cb7f-ea2f5f42d72b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Project Paths:\n","import os\n","from google.colab import drive; drive.mount('/content/drive') # Temporary for Colab\n","project_root = \"/content/drive/MyDrive/WoMAD/WoMAD/Notebooks/data\" # Google Colab-specific (different in the .py files and the repo)\n","\n","unprocessed_path = os.path.join(project_root, \"HCP_zipped\")\n","processed_path   = os.path.join(project_root, \"processed\")\n","model_ready_path = os.path.join(project_root, \"model_ready\")\n","\n","sub_list_txt_path = os.path.join(project_root, \"full_3T_task_subjects.txt\")\n","\n","# WoMAD-specific variables:\n","target_tasks = [\"WM\", \"EMOTION\", \"LANGUAGE\"]\n","\n","target_subtasks = {\n","    \"WM\"      : [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n","                 \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"],\n","    \"EMOTION\" : [\"fear\", \"neut\"],\n","    \"LANGUAGE\": [\"math\", \"story\"],\n","}\n","\n","TR = 0.72\n","\n","rest_tasks    = [\"REST1\", \"REST2\"]\n","run_direction = [\"LR\"   , \"RL\"]\n","\n","# Subjects with full 3T imaging protocol completed:\n","full_3T_task_subjects = []\n","\n","with open(sub_list_txt_path, \"r\") as file:\n","    raw_list = file.read()\n","    str_list = raw_list.strip().split(\",\")\n","    num_list = [int(subID.strip()) for subID in str_list if subID.strip()]\n","\n","full_3T_task_subjects = num_list\n","\n","# Model config dictionaries\n","lstm_config = {\n","    \"hidden_size\" : 128,\n","    \"num_layers\"  :   2,\n","    \"dropout\"     : 0.2\n","}\n","\n","fusion_config = {\n","    \"total_input_feats\" : 736, # Temporary: 360 parcels, 128 from the LSTM, and 248 guessing from the 4D network\n","    \"hidden_size\"       : 128\n","}\n","\n","training_loss_weights = {\n","    \"overall_loss_weight\" : 0.5,\n","    \"node_loss_weight\"    : 0.5\n","}\n","\n","# Randomly selected subject list for the pilot:\n","pilot_subjects = [283543, 180937, 379657, 145632, 100206, 270332, 707749, 454140, 194847, 185038]\n","\n","# Temporary variables for development:\n","dev_subjects = [100206, 100408]"]},{"cell_type":"code","source":["#GCA configurations\n","gca_config = {\n","    \"max_lag\": 1,\n","    \"significance_level\": 0.05,\n","    \"pairwise\": True\n","}\n","\n","#HMM configurations\n","hmm_config = {\n","    \"n_states\": 3,\n","    \"n_iter\": 100,\n","    \"covariance_type\": \"full\"\n","}\n"],"metadata":{"id":"MdgexxbClVKs","executionInfo":{"status":"ok","timestamp":1770150348970,"user_tz":300,"elapsed":3,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3X0Uv3awT3Z"},"source":["### Data Module"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"miyVrK7fwVwe","executionInfo":{"status":"ok","timestamp":1770150352009,"user_tz":300,"elapsed":52,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["# File Access:\n","def generate_paths(task : str = target_tasks[0],\n","                   run  : str = run_direction[0],\n","                   sub_list : list = dev_subjects):\n","    \"\"\"\n","    Uses the paths configured in WoMAD_config to create subject-specific paths.\n","\n","    Arguments:\n","        task             (str): The target task from HCP -> [W]orking [M]emory as default\n","        run              (str): RL or LR -> RL as our arbitrary default\n","        sub_list (list of int): List of target subject ID's as defined in the config file\n","\n","    Returns:\n","        Dictionary with this integer and tuple-of-strings format:\n","        paths = {\n","            subject_id (int) : (\"Main 'Results' file path\", \"EV file path\")\n","        }\n","    \"\"\"\n","    paths = {}\n","\n","    # General path format for each subject's directory:\n","    # (f\"../data/HCP_zipped/{subject-ID}/MNINonLinear/Results/\")\n","\n","    # General path format for subjects' task EV files:\n","    # (f\"../data/HCP_zipped/{subject-ID}/MNINonLinear/Results/tfMRI_{TASK}_{RUN}/EVs/\")\n","\n","    # List of target subjects: full_3T_task_subjects (imported from WoMAD_config)\n","    for subject in sub_list:\n","        subject_path    = f\"../data/HCP_zipped/{subject}/MNINonLinear/Results/\"\n","        subject_ev_path = f\"../data/HCP_zipped/{subject}/MNINonLinear/Results/tfMRI_{task}_{run}/EVs/\"\n","        paths[subject]  = (subject_path, subject_ev_path)\n","\n","    return paths\n","\n","\n","def load_data_from_path(task : str = target_tasks[0],\n","                        run  : str = run_direction[0],\n","                        subject : str = dev_subjects[0],\n","                        subtask : str = target_subtasks[\"WM\"][0]):\n","    \"\"\"\n","    Reads the contents of each subject's files.\n","\n","    Arguments:\n","        task    (str): The target task from HCP -> [W]orking [M]emory as default\n","        run     (str): RL or LR -> RL as our arbitrary default\n","        subject (int): ID of specific target subject\n","        subtask (str): The target subtask in string format -> Example: \"0bk_tools\"\n","\n","    Returns:\n","        Dictionary of {Subject: (Tuple of fMRI data)} and\n","        EV file contents assigned to the ev_file variable.\n","    \"\"\"\n","    try:\n","        paths = generate_paths(task = task, run = run)\n","        bold_ts_path = paths[subject][0] + f\"tfMRI_{task}_{run}/tfMRI_{task}_{run}_Atlas_MSMAll_hp0_clean_rclean_tclean.dtseries.nii\"\n","        ev_file_path = paths[subject][1] + f\"{subtask}.txt\"\n","\n","        with open(ev_file_path, \"r\") as ev:\n","            ev_file = ev.read()\n","\n","        bold_ts = nib.load(bold_ts_path)\n","        bold_data = bold_ts.get_fdata()\n","        bold_header = bold_ts.header\n","\n","        fmri_timeseries = {subject: (bold_ts, bold_header, bold_data)}\n","\n","        return fmri_timeseries, ev_file\n","\n","    except Exception as e:\n","        print(\"Error loading time series and EV files from path!\")\n","\n","\n","# Preprocessing:\n","## Parse and Isolate Trials\n","def isolate_trials(fmri_ts, ev_file, TR : float = 0.72):\n","    \"\"\"\n","    Parses through the data and isolates each task trial using EV files.\n","\n","    Input: The fMRI dictionary and EV file from load_data_from_path() function.\n","\n","    Returns:\n","        List of trials isolated using the ev_file.\n","    \"\"\"\n","    trial_list = []\n","\n","    for subject, (bold_ts, bold_header, bold_data) in fmri_ts.items():\n","        data_array = bold_data\n","\n","        try:\n","            ev_data = np.loadtxt(io.StringIO(ev_file))\n","        except ValueError:\n","            print(f\"Could not parse EV file for subject {subject}.\")\n","            continue\n","\n","        for onset, duration, _ in ev_data:\n","            start_idx = int(np.floor(onset / TR))\n","            end_idx = int(np.ceil((onset + duration) / TR))\n","            trial_data = data_array[:, start_idx:end_idx]\n","\n","            trial_list.append({\n","                \"subject\" : subject,\n","                \"onset\" : onset,\n","                \"duration\" : duration,\n","                \"data\" : trial_data\n","            })\n","\n","    return trial_list\n","\n","## Normalization\n","def normalize_data(data : np.ndarray, norm_mode: str = \"z_score\"):\n","    \"\"\"\n","    Normalizes a numpy array of fMRI time series data.\n","\n","    Arguments:\n","        data (np.ndarray): The time series data with shape (voxels, time_points)\n","        norm_mode   (str): Method of normalization (Z score, min/max, etc.)\n","\n","    Returns:\n","        Numpy array of normalized data.\n","    \"\"\"\n","    data = np.array(data)\n","\n","    if norm_mode == \"z_score\":\n","        ts_data_mean = np.mean(data, axis = 1, keepdims = True)\n","        ts_data_stdv = np.std(data , axis = 1, keepdims = True)\n","\n","        ts_data_stdv[ts_data_stdv == 0] = 1.0\n","\n","        normalized_ts_data = (data - ts_data_mean) / ts_data_stdv\n","\n","        return normalized_ts_data\n","\n","    elif norm_mode == \"min_max\":\n","        min_ts_data = np.min(data, axis = 1, keepdims = True)\n","        max_ts_data = np.max(data, axis = 1, keepdims = True)\n","\n","        range_ts_data = max_ts_data - min_ts_data\n","        range_ts_data[range_ts_data == 0] = 1.0\n","\n","        normalized_ts_data = (data - min_ts_data) / range_ts_data\n","\n","        return normalized_ts_data\n","\n","    else: # For now ...\n","        print(f\"Normalization mode '{norm_mode}' not defined.\\nReturning data as is.\")\n","        return data\n","\n","## Save to Pandas DataFrame\n","def save_to_df(trial_list : List[Dict[str, Any]],\n","               file_name : str,\n","               output_dir : str = processed_path):\n","    \"\"\"\n","    Converts the list of isolated trials to a Pandas DF and saves it to defined path.\n","\n","    Arguments:\n","        trial_list (list): List of {\"subject\", \"onset\", \"duration\", \"data\"} dictionaries.\n","        file_name   (str): Name of output file.\n","        output_dir  (str): Directory for saving the output file.\n","\n","    Saves the pd.DataFrame to output_dir.\n","    \"\"\"\n","    df_from_trial_ts = pd.DataFrame(trial_list)\n","\n","    os.makedirs(output_dir, exist_ok = True)\n","    save_path = os.path.join(output_dir, f\"{file_name}.pkl\")\n","\n","    df_from_trial_ts.to_pickle(save_path)\n","\n","    print(f\"Data saved to {save_path}\")\n","\n","    return df_from_trial_ts\n","\n","\n","# Initial Processing (with the WoMAD_data class):\n","class WoMAD_data(Dataset):\n","    def __init__(self,\n","                 task : str,\n","                 runs : list = run_direction,\n","                 subjects : list = dev_subjects,\n","                 output_dir : str = processed_path):\n","        \"\"\"\n","        Basic configuration of the dataset.\n","\n","        Arguments:\n","            task (str): The target task (\"WM\")\n","            runs (list): Run directions\n","            subjects (list): List of target subject IDs\n","            output_dir (str): Directory for saving processed data\n","        \"\"\"\n","        self.task = task\n","        self.runs = runs\n","        self.subjects = subjects\n","        self.output_dir = output_dir\n","\n","        self.data = []\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Returns the total number of subjects in the dataset.\n","        \"\"\"\n","        return len(self.data_paths)\n","\n","    def __getitem__(self, indx: int):\n","        \"\"\"\n","        Loads one trial's data and returns:\n","            - input timeseries (X)\n","            - overall target (Y_overall)\n","            - node target (Y_node)\n","        \"\"\"\n","        trial = self.data[indx]\n","\n","        # Input data with shape (target_nodes, timepoints)\n","        data_np = trial[\"data\"]\n","        data_tensor = torch.from_numpy(data_np).float()\n","\n","        # Overall target:\n","        # TODO: Add overall target components.\n","\n","        # Placeholder:\n","        overall_target_np = trial[\"stats\"][\"trial_mean\"]\n","        overall_target_tensor = torch.tensor([overall_target_np]).float()\n","\n","        # Node target with shape (target_nodes)\n","        node_target_np = trial[\"stats\"][\"mean_per_node\"]\n","        node_target_tensor = torch.from_numpy(node_target_np).float()\n","\n","        return data_tensor, overall_target_tensor, node_target_tensor\n","\n","    def _load_data(self):\n","        \"\"\"\n","        Load and parse the data using NIfTI and EV files.\n","        \"\"\"\n","        parsed_data = []\n","\n","        for subject in self.subject:\n","            for run in self.runs:\n","                paths = generate_paths(task = self.task, run = run,\n","                                       sub_list = [subject])\n","\n","                subtasks = target_subtasks.get(self.task, [])\n","\n","                for subtask in subtasks:\n","                    fmri_ts, ev_file = load_data_from_path(task = self.task,\n","                                                           run = run,\n","                                                           subject = subject,\n","                                                           subtask = subtask)\n","                    trial_list_subtask = isolate_trials(fmri_ts, ev_file)\n","\n","                    for trial_dict in trial_list_subtask:\n","                        trial_dict[\"run\"] = run\n","                        trial_dict[\"subtask\"] = subtask\n","                        parsed_data.append(trial_dict)\n","\n","        # TODO: Error handling inside the for loop.\n","\n","        self.data = parsed_data\n","\n","        return self.data\n","\n","    def basic_processing(self, norm_mode : str = \"z_score\",\n","                         file_name : str = \"processed_fMRI_data\"):\n","        \"\"\"\n","        Normalization and saving with normalize_data() and save_to_df().\n","        \"\"\"\n","        processed_trials = []\n","\n","        for trial in self.data:\n","            normalized_trial = normalize_data(trial[\"data\"], norm_mode = norm_mode)\n","\n","            trial[\"data\"] = normalized_trial\n","            trial[\"norm_mode\"] = norm_mode\n","            processed_trials.append(trial)\n","\n","        file_to_save_processed_data = f\"{file_name}_{self.task}_{norm_mode}\"\n","        self.processed_df = save_to_df(processed_trials,\n","                                       file_to_save_processed_data,\n","                                       self.output_dir)\n","\n","        self.data = processed_trials\n","\n","        return self.processed_df\n","\n","    def _calc_corr_matrix(self, trial_data: np.ndarray) -> dict:\n","        \"\"\"\n","        Calculates whole-brain and network-level correlation matrices for a single isolated trial.\n","\n","        NOTE:   Network-level correlations require network masks\n","                or specific voxel/parcel definitions.\n","                These have not yet been defined in the config files.\n","\n","        Arguments:\n","            trial_data (np.ndarray): The isolated trial's time series data.\n","\n","        Returns:\n","            A dictionary containing the calculated correlation matrix.\n","        \"\"\"\n","        # All voxels (whole_brain)\n","        whole_brain_corr_mat = np.corrcoef(trial_data)\n","\n","        whole_brain_corr_mat[np.isnan(whole_brain_corr_mat)] = 0\n","\n","        # TODO: Create network-level correlation\n","\n","        return {\n","            \"whole_brain\"  : whole_brain_corr_mat,\n","            \"network_level\": whole_brain_corr_mat   # Temp solution until the network-level is defined.\n","        }\n","\n","    def calc_func_connectivity(self):\n","        \"\"\"\n","        Calculates the correlation matrices for all isolated trials.\n","\n","        Returns:\n","            The updated list of trial dictionaries with the correlation matrices added.\n","        \"\"\"\n","        for trial in self.data:\n","            trial_ts_data = trial.get(\"data\") # Should be normalized\n","\n","            # Correlation matrix\n","            trial_corr_mat = self._calc_corr_matrix(trial_ts_data)\n","\n","            trial[\"corr_matrix\"] = trial_corr_mat\n","\n","        return self.data\n","\n","    def calc_basic_stats(self):\n","        \"\"\"\n","        Calculate basic statistics (mean and std) of\n","        the activity for isolated trials across all nodes/voxels.\n","\n","        Returns:\n","            Updated list of trial dictionaries with \"stats\" key added.\n","        \"\"\"\n","        for trial in self.data:\n","            trial_ts_data = trial.get(\"data\")\n","\n","            if trial_ts_data is None:\n","                continue\n","\n","            # Mean activity\n","            node_mean_activity = np.mean(trial_ts_data, axis = 1)\n","\n","            # Standard deviation\n","            node_std_activity = np.std(trial_ts_data, axis = 1)\n","\n","            # Overall average for entire trial\n","            trial_mean_activity = np.mean(node_mean_activity)\n","            trial_mean_of_std   = np.mean(node_std_activity)\n","\n","            trial[\"stats\"] = {\n","                \"mean_per_node\": node_mean_activity,\n","                \"std_per_node\" : node_std_activity,\n","                \"trial_mean\"   : trial_mean_activity,\n","                \"overall_std\"  : trial_mean_of_std\n","            }\n","\n","        return self.data\n","\n","    def visualize_correlations(self,\n","                               target_trial_idx: int = 0,\n","                               matrix_type: str = \"whole_brain\"):\n","        \"\"\"\n","        Generate a heatmap visualization for one trial's correlation matrix.\n","\n","        Arguments:\n","            target_trial_idx (int): Index of the trial in self.data\n","            matrix_type      (str): The type of matrix to plot\n","                                    \"whole_brain\" or \"network_level\"\n","        \"\"\"\n","        trial = self.data[target_trial_idx]\n","        corr_matrices = trial.get(\"corr_matrix\")\n","\n","        matrix_to_plot = corr_matrices.get(matrix_type)\n","\n","        plt.figure(figsize = (10, 8))\n","        sns.heatmap(matrix_to_plot,\n","                    cmap = \"RdBu_r\",\n","                    vmin = -1, vmax = 1,\n","                    square = True,\n","                    cbar_kws = {\"label\" : \"Pearson Correlation Coefficient\"})\n","\n","        subject_id = trial.get(\"subject\", \"N/A\")\n","\n","        task = trial.get(\"task\", \"N/A\")\n","        subtask = trial.get(\"subtask\", \"N/A\")\n","\n","        plt.title(f\"{matrix_type.title()} Correlation Matrix\\nSubject: {subject_id}, {task}: {subtask}, Trial {target_trial_idx}\")\n","        plt.xlabel(\"Node/Voxel Index\")\n","        plt.ylabel(\"Node/Voxel Index\")\n","\n","        plt.show()\n","\n","# TODO: Add function for \"validation set processing\" which can process non-HCP data."]},{"cell_type":"markdown","metadata":{"id":"IrYSUIinxCXs"},"source":["### Model Setup Module"]},{"cell_type":"markdown","source":["### The Dynamic Input module"],"metadata":{"id":"3Tj7naFdXzP7"}},{"cell_type":"code","source":["# Dynamic Adapter\n","TARGET_NODE_COUNT = 360       # 360 parcels based on HCP and Glasser parcellation\n","\n","class DynamicInput(nn.Module):\n","    \"\"\"\n","    This module handles input data with different number of voxels\n","    and adapts it for the modules (Info flow or Core) of the WoMAD model.\n","    \"\"\"\n","    def __init__(self, target_nodes: int = TARGET_NODE_COUNT):\n","        super().__init__()\n","        self.target_nodes = target_nodes\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Dynamically adapts the input data to defined dimension.\n","\n","        Input:\n","            x : 3D tensor with shape (batch, current_nodes, timepoints)\n","\n","        Output:\n","            The dynamically adapted 3D tensor with shape (batch, target_nodes, timepoints)\n","        \"\"\"\n","        batch, current_nodes, timepoints = x.shape\n","\n","        if current_nodes == self.target_nodes:\n","            return x\n","\n","        elif current_nodes > self.target_nodes:\n","            # TODO: Add voxel-to-parcel downsampling (requires a parcellation map)\n","\n","            # Current solution for downsampling: Adaptive Pooling (linear projection)\n","            x_reshaped = x.transpose(1, 2)\n","\n","            x_pooled = F.adaptive_avg_pool1d(x_reshaped, self.target_nodes)\n","\n","            x_pooled_out = x_pooled.transpose(1, 2)\n","\n","            return x_pooled_out\n","\n","        else: # current_nodes < self.target_nodes\n","            required_padding = self.target_nodes - current_nodes\n","\n","            x_padded = F.pad(x, (0, 0, 0, required_padding))\n","\n","            return x_padded"],"metadata":{"id":"6UmT9Fq1XxwW","executionInfo":{"status":"ok","timestamp":1770150360291,"user_tz":300,"elapsed":6,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kTxQZ2r0Tlj"},"source":["#### Info flow submodule\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"q0BQTRbo0TOf","executionInfo":{"status":"ok","timestamp":1770150362792,"user_tz":300,"elapsed":2,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["#GCA\n","from scipy import stats\n","from typing import Tuple, Dict, Optional, Any\n","import warnings\n","\n","\n","class GrangerCausalityAnalysis:\n","    def __init__(self, config: dict):\n","        \"\"\"\n","        Sets up the complete WoMAD model with all modules and submodules.\n","        (Each module and submodule includes a dynamic input layer that matches the size of input.)\n","        \"\"\"\n","        # Information Flow Module\n","        ## Effective Connectivity: GCA\n","        ## Dynamic Functional Connectivity: HMM\n","        ## Final info-flow manifold: Temporal GNN\n","\n","    def __init__(self,\n","                 max_lag: int = 1,\n","                 significance_level: float = 0.05):\n","        \"\"\"\n","        Initialize GCA analysis.\n","\n","        Args:\n","            max_lag: Maximum lag for VAR model (default=1 for fMRI)\n","            significance_level: P-value threshold for significance testing\n","        \"\"\"\n","        self.max_lag = max_lag\n","        self.significance_level = significance_level\n","        self.adjacency_matrix = None\n","        self.f_statistics = None\n","        self.p_values = None\n","\n","    def _create_lagged_data(self,\n","                            data: np.ndarray,\n","                            lag: int) -> Tuple[np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Create lagged versions of time series for VAR modeling.\n","\n","        Args:\n","            data: Time series (n_nodes, n_timepoints)\n","            lag: Number of time lags\n","\n","        Returns:\n","            X: Lagged predictors (n_samples, n_nodes * lag)\n","            Y: Target values (n_samples, n_nodes)\n","        \"\"\"\n","        n_nodes, n_timepoints = data.shape\n","        n_samples = n_timepoints - lag\n","\n","        # Target: values at time t\n","        Y = data[:, lag:].T  # (n_samples, n_nodes)\n","\n","        # Predictors: values at time t-1, t-2, ..., t-lag\n","        X_list = []\n","        for l in range(1, lag + 1):\n","            X_list.append(data[:, lag-l:n_timepoints-l].T)\n","        X = np.hstack(X_list)  # (n_samples, n_nodes * lag)\n","\n","        return X, Y\n","\n","    def _fit_var_model(self,\n","                       X: np.ndarray,\n","                       y: np.ndarray) -> Tuple[np.ndarray, float]:\n","        \"\"\"\n","        Fit VAR model using OLS regression.\n","\n","        Args:\n","            X: Predictor matrix\n","            y: Target vector\n","\n","        Returns:\n","            coefficients: Regression coefficients\n","            residual_variance: Variance of residuals\n","        \"\"\"\n","        # Add intercept\n","        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n","\n","        # OLS solution: beta = (X'X)^(-1) X'y\n","        try:\n","            XtX_inv = np.linalg.pinv(X_with_intercept.T @ X_with_intercept)\n","            coefficients = XtX_inv @ X_with_intercept.T @ y\n","\n","            # Calculate residuals\n","            y_pred = X_with_intercept @ coefficients\n","            residuals = y - y_pred\n","            residual_variance = np.var(residuals)\n","\n","            return coefficients, residual_variance\n","        except np.linalg.LinAlgError:\n","            return None, np.inf\n","\n","    def _granger_causality_test(self,\n","                                X_full: np.ndarray,\n","                                X_reduced: np.ndarray,\n","                                y: np.ndarray) -> Tuple[float, float]:\n","        \"\"\"\n","        Perform F-test for Granger causality.\n","\n","        Args:\n","            X_full: Full model predictors (includes candidate cause)\n","            X_reduced: Reduced model predictors (excludes candidate cause)\n","            y: Target time series\n","\n","        Returns:\n","            f_statistic: F-test statistic\n","            p_value: P-value of the test\n","        \"\"\"\n","        n_samples = len(y)\n","\n","        # Fit full model\n","        _, rss_full = self._fit_var_model(X_full, y)\n","\n","        # Fit reduced model\n","        _, rss_reduced = self._fit_var_model(X_reduced, y)\n","\n","        if rss_full == np.inf or rss_reduced == np.inf:\n","            return 0.0, 1.0\n","\n","        # Degrees of freedom\n","        df_full = X_full.shape[1] + 1  # +1 for intercept\n","        df_reduced = X_reduced.shape[1] + 1\n","        df_diff = df_full - df_reduced\n","        df_residual = n_samples - df_full\n","\n","        if df_residual <= 0 or df_diff <= 0:\n","            return 0.0, 1.0\n","\n","        # F-statistic\n","        if rss_full > 0:\n","            f_stat = ((rss_reduced - rss_full) / df_diff) / (rss_full / df_residual)\n","            f_stat = max(0, f_stat)  # Ensure non-negative\n","        else:\n","            f_stat = 0.0\n","\n","        # P-value\n","        p_value = 1 - stats.f.cdf(f_stat, df_diff, df_residual)\n","\n","        return f_stat, p_value\n","\n","    def compute_connectivity(self,\n","                            data: np.ndarray,\n","                            verbose: bool = False) -> Dict[str, np.ndarray]:\n","        \"\"\"\n","        Compute pairwise Granger causality for all node pairs.\n","\n","        Args:\n","            data: fMRI time series (n_nodes, n_timepoints)\n","            verbose: Print progress\n","\n","        Returns:\n","            Dict with adjacency matrix, F-statistics, and p-values\n","        \"\"\"\n","        n_nodes, n_timepoints = data.shape\n","\n","        # Initialize output matrices\n","        self.f_statistics = np.zeros((n_nodes, n_nodes))\n","        self.p_values = np.ones((n_nodes, n_nodes))\n","\n","        # Create lagged data\n","        X_all, Y = self._create_lagged_data(data, self.max_lag)\n","\n","        # Test each directed pair\n","        for target in range(n_nodes):\n","            y = Y[:, target]\n","\n","            for source in range(n_nodes):\n","                if source == target:\n","                    continue\n","\n","                # Full model: all nodes predict target\n","                X_full = X_all\n","\n","                # Reduced model: exclude source node\n","                source_cols = [source + i * n_nodes for i in range(self.max_lag)]\n","                other_cols = [c for c in range(X_all.shape[1]) if c not in source_cols]\n","                X_reduced = X_all[:, other_cols]\n","\n","                # Granger causality test\n","                f_stat, p_val = self._granger_causality_test(X_full, X_reduced, y)\n","\n","                self.f_statistics[source, target] = f_stat\n","                self.p_values[source, target] = p_val\n","\n","        # Create binary adjacency matrix based on significance\n","        self.adjacency_matrix = (self.p_values < self.significance_level).astype(float)\n","        np.fill_diagonal(self.adjacency_matrix, 0)\n","\n","        return {\n","            'adjacency': self.adjacency_matrix,\n","            'f_statistics': self.f_statistics,\n","            'p_values': self.p_values\n","        }\n","\n","    def get_hub_nodes(self, top_k: int = 10) -> Dict[str, List[int]]:\n","        \"\"\"\n","        Identify hub nodes based on connectivity.\n","\n","        Args:\n","            top_k: Number of top hubs to return\n","\n","        Returns:\n","            Dict with hub nodes for outgoing and incoming connections\n","        \"\"\"\n","        if self.adjacency_matrix is None:\n","            raise ValueError(\"Run compute_connectivity first\")\n","\n","        out_degree = np.sum(self.adjacency_matrix, axis=1)\n","        in_degree = np.sum(self.adjacency_matrix, axis=0)\n","\n","        top_k = min(top_k, len(out_degree))\n","\n","        return {\n","            'hub_nodes_out': np.argsort(out_degree)[-top_k:][::-1].tolist(),\n","            'hub_nodes_in': np.argsort(in_degree)[-top_k:][::-1].tolist(),\n","            'out_degree': out_degree,\n","            'in_degree': in_degree\n","        }\n","\n","\n","def compute_gca_for_trial(trial_data: np.ndarray,\n","                          max_lag: int = 1,\n","                          significance_level: float = 0.05) -> Dict:\n","    \"\"\"\n","    Convenience function to compute GCA for a single trial.\n","\n","    Args:\n","        trial_data: fMRI data (n_nodes, n_timepoints)\n","        max_lag: VAR model lag\n","        significance_level: P-value threshold\n","\n","    Returns:\n","        Dict with GCA results and summary statistics\n","    \"\"\"\n","    gca = GrangerCausalityAnalysis(max_lag=max_lag,\n","                                   significance_level=significance_level)\n","    results = gca.compute_connectivity(trial_data)\n","    hub_info = gca.get_hub_nodes()\n","\n","    n_significant = int(np.sum(results['adjacency']))\n","    n_possible = results['adjacency'].shape[0] * (results['adjacency'].shape[0] - 1)\n","\n","    return {\n","        'adjacency': results['adjacency'],\n","        'f_statistics': results['f_statistics'],\n","        'p_values': results['p_values'],\n","        'summary': {\n","            'n_significant': n_significant,\n","            'density': n_significant / n_possible if n_possible > 0 else 0,\n","            'hub_nodes_out': hub_info['hub_nodes_out'],\n","            'hub_nodes_in': hub_info['hub_nodes_in']\n","        }\n","    }"]},{"cell_type":"code","source":["#HMM\n","!pip install --quiet hmmlearn\n","from hmmlearn import hmm\n","\n","from sklearn.decomposition import PCA\n","\n","def run_hmm_stating(trial_data: np.ndarray,\n","                    n_states: int = 3,\n","                    n_iter: int = 100,\n","                    n_pca_components: int = None) -> Dict:\n","    \"\"\"\n","    Identifies hidden brain states using Hidden Markov Model.\n","\n","    Args:\n","        trial_data (np.ndarray): fMRI data (n_nodes, n_timepoints).\n","        n_states: Number of hidden states to detect.\n","        n_iter: Number of EM iterations.\n","        n_pca_components: Number of principal components to use for PCA.\n","\n","    Returns:\n","        Dict with state_sequence, state_means, transition_matrix\n","    \"\"\"\n","\n","    #HMM expects (n_samples, n_features) = (timepoints, nodes)\n","    X = trial_data.T\n","    n_samples, n_features = X.shape\n","\n","    # Use PCA if features > samples to avoid degenerate solution\n","    pca_used = False\n","    if n_pca_components is None:\n","        # Keep n_features such that model parameters < data points\n","        max_components = min(n_samples - 1, n_features, 20)\n","        if n_features > max_components:\n","            n_pca_components = max_components\n","\n","    if n_pca_components and n_pca_components < n_features:\n","        pca = PCA(n_components=n_pca_components, random_state=42)\n","        X = pca.fit_transform(X)\n","        pca_used = True\n","\n","    # Fit HMM with diagonal covariance for efficiency\n","    model = hmm.GaussianHMM(\n","        n_components=n_states,\n","        covariance_type=\"diag\",\n","        n_iter=n_iter,\n","        random_state=42\n","    )\n","\n","    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\", n_iter=100)\n","\n","    model.fit(X)\n","\n","    state_sequence = model.predict(X)\n","\n","    return {\n","        'state_sequence': state_sequence,\n","        'state_means': model.means_,\n","        'transition_matrix': model.transmat_,\n","        'n_states': n_states,\n","        'pca_used': pca_used,\n","        'n_features_used': X.shape[1]\n","    }\n","\n","def compute_state_connectivity(trial_data: np.ndarray,\n","                               state_sequence: np.ndarray) -> Dict[int, np.ndarray]:\n","    \"\"\"\n","    Compute functional connectivity matrix for each HMM state.\n","\n","    Args:\n","        trial_data: fMRI data (n_nodes, n_timepoints)\n","        state_sequence: HMM state labels for each timepoint\n","\n","    Returns:\n","        Dict mapping state_id -> correlation matrix\n","    \"\"\"\n","    unique_states = np.unique(state_sequence)\n","    state_connectivity = {}\n","\n","    for state in unique_states:\n","        state_mask = state_sequence == state\n","        if np.sum(state_mask) < 2:\n","            continue\n","\n","        # Extract timepoints for this state\n","        state_data = trial_data[:, state_mask]\n","\n","        # Compute correlation matrix\n","        corr_matrix = np.corrcoef(state_data)\n","        corr_matrix[np.isnan(corr_matrix)] = 0\n","\n","        state_connectivity[int(state)] = corr_matrix\n","\n","    return state_connectivity"],"metadata":{"id":"AckY1XRgiwJM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770150411862,"user_tz":300,"elapsed":8254,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}},"outputId":"6ae61b87-a720-4a2b-d5d5-2a0187008058"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/166.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["#Topological Similarity & Graph Overlap Analysis\n","class TopologicalSimilarity:\n","    \"\"\"\n","    Compute and compare topological properties of brain connectivity graphs.\n","\n","    This implements Step 5 of the Information Flow Module:\n","    Compare graph architecture using key topological metrics.\n","\n","    Metrics computed:\n","    - Degree distribution\n","    - Clustering coefficient\n","    - Global/Local efficiency\n","    - Modularity\n","    \"\"\"\n","\n","    def __init__(self, n_nodes: int = 360):\n","        self.n_nodes = n_nodes\n","\n","    def degree_distribution(self, adj: np.ndarray) -> Dict[str, Any]:\n","        \"\"\"Compute degree distribution statistics.\"\"\"\n","        # Binarize\n","        binary_adj = (adj != 0).astype(int)\n","        np.fill_diagonal(binary_adj, 0)\n","\n","        # Degree (sum of connections per node)\n","        degree = np.sum(binary_adj, axis=1)\n","\n","        return {\n","            'degrees': degree,\n","            'mean_degree': float(np.mean(degree)),\n","            'std_degree': float(np.std(degree)),\n","            'max_degree': int(np.max(degree)),\n","            'min_degree': int(np.min(degree))\n","        }\n","\n","    def clustering_coefficient(self, adj: np.ndarray) -> Dict[str, Any]:\n","        \"\"\"Compute clustering coefficient for each node.\"\"\"\n","        binary_adj = (adj != 0).astype(int)\n","        np.fill_diagonal(binary_adj, 0)\n","\n","        n = binary_adj.shape[0]\n","        clustering = np.zeros(n)\n","\n","        for i in range(n):\n","            neighbors = np.where(binary_adj[i] > 0)[0]\n","            k = len(neighbors)\n","\n","            if k < 2:\n","                clustering[i] = 0\n","                continue\n","\n","            # Count edges between neighbors\n","            subgraph = binary_adj[np.ix_(neighbors, neighbors)]\n","            n_triangles = np.sum(subgraph) / 2\n","            max_triangles = k * (k - 1) / 2\n","\n","            clustering[i] = n_triangles / max_triangles if max_triangles > 0 else 0\n","\n","        return {\n","            'node_clustering': clustering,\n","            'mean_clustering': float(np.mean(clustering)),\n","            'std_clustering': float(np.std(clustering))\n","        }\n","\n","    def global_efficiency(self, adj: np.ndarray) -> float:\n","        \"\"\"Compute global efficiency of the network.\"\"\"\n","        binary_adj = (adj != 0).astype(int)\n","        np.fill_diagonal(binary_adj, 0)\n","\n","        n = binary_adj.shape[0]\n","\n","        # Floyd-Warshall for shortest paths\n","        dist = np.where(binary_adj > 0, 1, np.inf)\n","        np.fill_diagonal(dist, 0)\n","\n","        for k in range(n):\n","            for i in range(n):\n","                for j in range(n):\n","                    if dist[i, k] + dist[k, j] < dist[i, j]:\n","                        dist[i, j] = dist[i, k] + dist[k, j]\n","\n","        # Efficiency = 1/distance (excluding self-connections)\n","        with np.errstate(divide='ignore'):\n","            inv_dist = 1.0 / dist\n","        inv_dist[np.isinf(inv_dist)] = 0\n","        np.fill_diagonal(inv_dist, 0)\n","\n","        return float(np.sum(inv_dist) / (n * (n - 1)))\n","\n","    def modularity(self, adj: np.ndarray, n_communities: int = None) -> Dict[str, Any]:\n","        \"\"\"Compute modularity using spectral clustering.\"\"\"\n","        from sklearn.cluster import SpectralClustering\n","\n","        binary_adj = (adj != 0).astype(float)\n","        np.fill_diagonal(binary_adj, 0)\n","\n","        # Make symmetric\n","        symmetric_adj = (binary_adj + binary_adj.T) / 2\n","\n","        # Determine number of communities\n","        if n_communities is None:\n","            n_communities = min(5, max(2, int(np.sqrt(adj.shape[0] / 2))))\n","\n","        # Spectral clustering\n","        try:\n","            clustering = SpectralClustering(\n","                n_clusters=n_communities,\n","                affinity='precomputed',\n","                random_state=42,\n","                assign_labels='kmeans'\n","            )\n","            labels = clustering.fit_predict(symmetric_adj + 0.01)\n","        except:\n","            labels = np.zeros(adj.shape[0], dtype=int)\n","\n","        # Calculate modularity Q\n","        m = np.sum(symmetric_adj) / 2\n","        if m == 0:\n","            return {'modularity': 0.0, 'community_labels': labels, 'n_communities': n_communities}\n","\n","        k = np.sum(symmetric_adj, axis=1)\n","        Q = 0\n","        for i in range(adj.shape[0]):\n","            for j in range(adj.shape[0]):\n","                if labels[i] == labels[j]:\n","                    Q += symmetric_adj[i, j] - (k[i] * k[j]) / (2 * m)\n","        Q /= (2 * m)\n","\n","        return {\n","            'modularity': float(Q),\n","            'community_labels': labels,\n","            'n_communities': len(np.unique(labels))\n","        }\n","\n","\n","class GraphOverlapAnalysis:\n","    \"\"\"\n","    Analyze overlap between connectivity graphs from different tasks.\n","\n","    This implements Step 4 of the Information Flow Module:\n","    Compare graphs to quantify WM involvement in non-WM tasks.\n","\n","    Metrics:\n","    - Jaccard Index (edge-wise overlap)\n","    - Pearson correlation (weighted similarity)\n","    - WM involvement score\n","    \"\"\"\n","\n","    def __init__(self, n_nodes: int = 360):\n","        self.n_nodes = n_nodes\n","\n","    def jaccard_index(self,\n","                      adj1: np.ndarray,\n","                      adj2: np.ndarray,\n","                      threshold_percentile: float = None) -> Dict[str, float]:\n","        \"\"\"\n","        Compute Jaccard Index for edge-wise overlap.\n","\n","        Jaccard = |A ∩ B| / |A ∪ B|\n","        \"\"\"\n","        # Binarize\n","        if threshold_percentile and np.any(adj1 > 0) and np.any(adj2 > 0):\n","            thresh1 = np.percentile(adj1[adj1 > 0], threshold_percentile)\n","            thresh2 = np.percentile(adj2[adj2 > 0], threshold_percentile)\n","            binary1 = (adj1 >= thresh1).astype(int)\n","            binary2 = (adj2 >= thresh2).astype(int)\n","        else:\n","            binary1 = (adj1 != 0).astype(int)\n","            binary2 = (adj2 != 0).astype(int)\n","\n","        np.fill_diagonal(binary1, 0)\n","        np.fill_diagonal(binary2, 0)\n","\n","        # Jaccard calculation\n","        intersection = np.sum(binary1 & binary2)\n","        union = np.sum(binary1 | binary2)\n","\n","        jaccard = intersection / union if union > 0 else 0\n","\n","        return {\n","            'jaccard_index': float(jaccard),\n","            'intersection_edges': int(intersection),\n","            'union_edges': int(union),\n","            'edges_in_graph1': int(np.sum(binary1)),\n","            'edges_in_graph2': int(np.sum(binary2))\n","        }\n","\n","    def pearson_correlation(self,\n","                           adj1: np.ndarray,\n","                           adj2: np.ndarray) -> Dict[str, float]:\n","        \"\"\"Compute Pearson correlation between flattened adjacency matrices.\"\"\"\n","        # Get upper triangle (excluding diagonal)\n","        mask = np.triu(np.ones_like(adj1, dtype=bool), k=1)\n","\n","        vec1 = adj1[mask]\n","        vec2 = adj2[mask]\n","\n","        if np.std(vec1) == 0 or np.std(vec2) == 0:\n","            return {'pearson_r': 0.0, 'p_value': 1.0}\n","\n","        r, p = stats.pearsonr(vec1, vec2)\n","\n","        return {\n","            'pearson_r': float(r),\n","            'p_value': float(p)\n","        }\n","\n","    def compute_overlap(self,\n","                        adj1: np.ndarray,\n","                        adj2: np.ndarray,\n","                        threshold_percentile: float = 10.0) -> Dict[str, Any]:\n","        \"\"\"Compute all overlap metrics between two graphs.\"\"\"\n","        jaccard = self.jaccard_index(adj1, adj2, threshold_percentile)\n","        correlation = self.pearson_correlation(adj1, adj2)\n","\n","        return {\n","            'jaccard': jaccard,\n","            'correlation': correlation\n","        }\n","\n","    def wm_involvement_score(self,\n","                             wm_adjacency: np.ndarray,\n","                             task_adjacency: np.ndarray) -> Dict[str, float]:\n","        \"\"\"\n","        Calculate WM involvement score for a non-WM task.\n","\n","        Score = fraction of WM network edges present in task network\n","        \"\"\"\n","        wm_binary = (wm_adjacency != 0).astype(int)\n","        task_binary = (task_adjacency != 0).astype(int)\n","\n","        np.fill_diagonal(wm_binary, 0)\n","        np.fill_diagonal(task_binary, 0)\n","\n","        wm_edges = np.sum(wm_binary)\n","        overlap = np.sum(wm_binary & task_binary)\n","\n","        score = overlap / wm_edges if wm_edges > 0 else 0\n","\n","        return {\n","            'wm_involvement_score': float(score),\n","            'wm_edges_in_task': int(overlap),\n","            'total_wm_edges': int(wm_edges),\n","            'total_task_edges': int(np.sum(task_binary))\n","        }\n","\n","    def compare_multiple_tasks(self,\n","                               task_adjacencies: Dict[str, np.ndarray]) -> 'pd.DataFrame':\n","        \"\"\"Compare all pairs of task graphs.\"\"\"\n","        import pandas as pd\n","\n","        tasks = list(task_adjacencies.keys())\n","        results = []\n","\n","        for i, task1 in enumerate(tasks):\n","            for task2 in tasks[i+1:]:\n","                overlap = self.compute_overlap(\n","                    task_adjacencies[task1],\n","                    task_adjacencies[task2]\n","                )\n","                results.append({\n","                    'task1': task1,\n","                    'task2': task2,\n","                    'jaccard_index': overlap['jaccard']['jaccard_index'],\n","                    'pearson_r': overlap['correlation']['pearson_r']\n","                })\n","\n","        return pd.DataFrame(results)"],"metadata":{"id":"9ywnHHFon1Aw","executionInfo":{"status":"ok","timestamp":1770150417522,"user_tz":300,"elapsed":52,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["#Temporal GNN & Info Flow Integration\n","class TemporalGraphBuilder:\n","    \"\"\"\n","    Build temporal graph sequences from fMRI data for GNN processing.\n","\n","    Creates PyG Data objects with:\n","    - Node features from BOLD signal\n","    - Edge connections from GCA adjacency\n","    - Edge weights from F-statistics\n","    - HMM state labels as graph attributes\n","    \"\"\"\n","\n","    def __init__(self, n_nodes: int = 360):\n","        self.n_nodes = n_nodes\n","\n","    def build_temporal_graphs(self,\n","                              trial_data: np.ndarray,\n","                              gca_result: Dict,\n","                              hmm_result: Dict) -> List:\n","        \"\"\"\n","        Build a sequence of graphs for each timepoint.\n","\n","        Args:\n","            trial_data: (n_nodes, n_timepoints) BOLD data\n","            gca_result: Output from compute_gca_for_trial()\n","            hmm_result: Output from run_hmm_analysis()\n","\n","        Returns:\n","            List of PyG Data objects\n","        \"\"\"\n","        if not HAS_PYG:\n","            raise ImportError(\"torch_geometric required. Install with: pip install torch_geometric\")\n","\n","        n_nodes, n_timepoints = trial_data.shape\n","        adjacency = gca_result['adjacency']\n","        f_stats = gca_result['f_statistics']\n","        state_sequence = hmm_result['state_sequence']\n","\n","        # Convert adjacency to edge_index format\n","        edge_indices = np.array(np.where(adjacency > 0))\n","        edge_index = torch.tensor(edge_indices, dtype=torch.long)\n","\n","        # Edge weights from F-statistics\n","        edge_weights = f_stats[adjacency > 0]\n","        edge_attr = torch.tensor(edge_weights, dtype=torch.float).unsqueeze(1)\n","\n","        # Build graph for each timepoint\n","        graphs = []\n","        for t in range(n_timepoints):\n","            # Node features = BOLD signal at this timepoint\n","            x = torch.tensor(trial_data[:, t], dtype=torch.float).unsqueeze(1)\n","\n","            # Create PyG Data object\n","            data = Data(\n","                x=x,\n","                edge_index=edge_index,\n","                edge_attr=edge_attr,\n","                hmm_state=int(state_sequence[t]) if t < len(state_sequence) else 0\n","            )\n","            graphs.append(data)\n","\n","        return graphs\n","\n","\n","class TemporalGNN(nn.Module):\n","    \"\"\"\n","    Temporal Graph Neural Network for learning information flow patterns.\n","\n","    Architecture:\n","    1. GAT layers for spatial message passing (across brain regions)\n","    2. LSTM for temporal dynamics (across timepoints)\n","    3. Graph-level readout for trial embedding\n","\n","    This is Step 3 of the Information Flow Module.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 in_channels: int = 1,\n","                 hidden_channels: int = 64,\n","                 out_channels: int = 32,\n","                 num_gat_layers: int = 2,\n","                 num_heads: int = 4,\n","                 lstm_hidden: int = 128,\n","                 lstm_layers: int = 2,\n","                 dropout: float = 0.2):\n","        super(TemporalGNN, self).__init__()\n","\n","        if not HAS_PYG:\n","            raise ImportError(\"torch_geometric required\")\n","\n","        self.hidden_channels = hidden_channels\n","        self.out_channels = out_channels\n","\n","        # GAT layers for spatial processing\n","        self.gat_layers = nn.ModuleList()\n","\n","        # First GAT layer\n","        self.gat_layers.append(\n","            GATConv(in_channels, hidden_channels, heads=num_heads, concat=True, dropout=dropout)\n","        )\n","\n","        # Middle GAT layers\n","        for _ in range(num_gat_layers - 2):\n","            self.gat_layers.append(\n","                GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, concat=True, dropout=dropout)\n","            )\n","\n","        # Final GAT layer\n","        if num_gat_layers > 1:\n","            self.gat_layers.append(\n","                GATConv(hidden_channels * num_heads, out_channels, heads=1, concat=False, dropout=dropout)\n","            )\n","\n","        # LSTM for temporal processing\n","        self.lstm = nn.LSTM(\n","            input_size=out_channels,\n","            hidden_size=lstm_hidden,\n","            num_layers=lstm_layers,\n","            batch_first=True,\n","            dropout=dropout if lstm_layers > 1 else 0\n","        )\n","\n","        # Output projection\n","        self.output_proj = nn.Linear(lstm_hidden, out_channels)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, graph_sequence: List) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Process a sequence of temporal graphs.\n","\n","        Args:\n","            graph_sequence: List of PyG Data objects (one per timepoint)\n","\n","        Returns:\n","            Dict with node embeddings and graph embedding\n","        \"\"\"\n","        # Process each graph through GAT layers\n","        node_embeddings_sequence = []\n","\n","        for data in graph_sequence:\n","            x = data.x\n","            edge_index = data.edge_index\n","\n","            # Apply GAT layers\n","            for i, gat_layer in enumerate(self.gat_layers):\n","                x = gat_layer(x, edge_index)\n","                if i < len(self.gat_layers) - 1:\n","                    x = F.elu(x)\n","                    x = self.dropout(x)\n","\n","            node_embeddings_sequence.append(x)\n","\n","        # Stack temporal sequence: (n_nodes, n_timepoints, out_channels)\n","        node_temporal = torch.stack(node_embeddings_sequence, dim=1)\n","\n","        # Apply LSTM across time for each node\n","        n_nodes = node_temporal.shape[0]\n","        lstm_out, (h_n, _) = self.lstm(node_temporal)\n","\n","        # Final node embeddings (last LSTM hidden state)\n","        node_embeddings = self.output_proj(h_n[-1])\n","\n","        # Graph-level embedding (mean pooling over nodes)\n","        graph_embedding = torch.mean(node_embeddings, dim=0, keepdim=True)\n","\n","        return {\n","            'node_embeddings': node_embeddings,\n","            'graph_embedding': graph_embedding,\n","            'temporal_features': lstm_out\n","        }\n","\n","\n","class WoMAD_info_flow(nn.Module):\n","    \"\"\"\n","    Complete Information Flow Module for WoMAD.\n","\n","    Integrates:\n","    1. GCA (Granger Causality Analysis) - effective connectivity\n","    2. HMM (Hidden Markov Model) - brain state detection\n","    3. Temporal GNN - spatiotemporal learning\n","    4. Graph Overlap - task comparison\n","    5. Topological Similarity - network metrics\n","    \"\"\"\n","\n","    def __init__(self, config: dict):\n","        super(WoMAD_info_flow, self).__init__()\n","\n","        self.target_nodes = config.get(\"target_nodes\", 360)\n","        self.hidden_size = config.get(\"hidden_size\", 128)\n","\n","        # GCA parameters\n","        self.gca_max_lag = config.get(\"gca_max_lag\", 1)\n","        self.gca_significance = config.get(\"gca_significance\", 0.05)\n","\n","        # HMM parameters\n","        self.hmm_n_states = config.get(\"hmm_n_states\", 3)\n","\n","        # Temporal GNN (if available)\n","        if HAS_PYG:\n","            self.temporal_gnn = TemporalGNN(\n","                in_channels=1,\n","                hidden_channels=64,\n","                out_channels=self.hidden_size,\n","                num_gat_layers=2,\n","                num_heads=4\n","            )\n","        else:\n","            self.temporal_gnn = None\n","\n","        # Graph builder\n","        self.graph_builder = TemporalGraphBuilder(self.target_nodes) if HAS_PYG else None\n","\n","        # Analysis modules (non-learnable)\n","        self.gca_analyzer = GrangerCausalityAnalysis(\n","            max_lag=self.gca_max_lag,\n","            significance_level=self.gca_significance\n","        )\n","        self.topo_analyzer = TopologicalSimilarity(self.target_nodes)\n","        self.overlap_analyzer = GraphOverlapAnalysis(self.target_nodes)\n","\n","    def analyze_trial(self, trial_data: np.ndarray) -> Dict[str, Any]:\n","        \"\"\"\n","        Run complete information flow analysis on a single trial.\n","\n","        Args:\n","            trial_data: fMRI data (n_nodes, n_timepoints)\n","\n","        Returns:\n","            Dict with all analysis results\n","        \"\"\"\n","        results = {}\n","\n","        # Step 1: GCA\n","        gca_result = compute_gca_for_trial(\n","            trial_data,\n","            max_lag=self.gca_max_lag,\n","            significance_level=self.gca_significance\n","        )\n","        results['gca'] = gca_result\n","\n","        # Step 2: HMM\n","        hmm_result = run_hmm_analysis(trial_data, n_states=self.hmm_n_states)\n","        results['hmm'] = hmm_result\n","\n","        # Step 3: State-wise connectivity\n","        state_connectivity = compute_state_connectivity(\n","            trial_data, hmm_result['state_sequence']\n","        )\n","        results['state_connectivity'] = state_connectivity\n","\n","        # Step 4: Topological metrics\n","        topo_metrics = {\n","            'degree': self.topo_analyzer.degree_distribution(gca_result['adjacency']),\n","            'clustering': self.topo_analyzer.clustering_coefficient(gca_result['adjacency']),\n","            'global_efficiency': self.topo_analyzer.global_efficiency(gca_result['adjacency']),\n","            'modularity': self.topo_analyzer.modularity(gca_result['adjacency'])\n","        }\n","        results['topology'] = topo_metrics\n","\n","        return results\n","\n","    def forward(self, trial_data: torch.Tensor) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Forward pass through the learnable components.\n","\n","        Args:\n","            trial_data: Tensor (n_nodes, n_timepoints)\n","\n","        Returns:\n","            Dict with embeddings from Temporal GNN\n","        \"\"\"\n","        # Convert to numpy for GCA/HMM\n","        data_np = trial_data.detach().cpu().numpy()\n","\n","        # Run analysis\n","        gca_result = compute_gca_for_trial(data_np, max_lag=self.gca_max_lag)\n","        hmm_result = run_hmm_analysis(data_np, n_states=self.hmm_n_states)\n","\n","        # Build graphs and run GNN\n","        if self.temporal_gnn is not None and self.graph_builder is not None:\n","            graphs = self.graph_builder.build_temporal_graphs(data_np, gca_result, hmm_result)\n","            gnn_output = self.temporal_gnn(graphs)\n","            return gnn_output\n","        else:\n","            # Return placeholder if GNN not available\n","            return {\n","                'node_embeddings': torch.zeros(data_np.shape[0], self.hidden_size),\n","                'graph_embedding': torch.zeros(1, self.hidden_size)\n","            }"],"metadata":{"id":"3_hG3SdrlCQj","executionInfo":{"status":"ok","timestamp":1770150424285,"user_tz":300,"elapsed":116,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.nn import GATConv\n","\n","class WoMAD_info_flow(nn.Module):\n","    def __init__(self, config: dict):\n","        super(WoMAD_info_flow, self).__init__()\n","        self.target_nodes = 360 # Defined in your config [cite: 374, 442]\n","        self.hidden_size = 128 # Defined in your LSTM config [cite: 61, 439]\n","\n","        # The GNN Layer: Learns directed attention between parcels\n","        # in_channels: BOLD activity at each node\n","        # out_channels: The hidden representation of \"info flow\"\n","        self.gnn_manifold = GATConv(in_channels=1,\n","                                    out_channels=self.hidden_size,\n","                                    heads=4,\n","                                    concat=True)\n","\n","        # Linear layer to condense multi-head attention back to hidden_size\n","        self.post_gnn = nn.Linear(self.hidden_size * 4, self.hidden_size)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        \"\"\"\n","        x: BOLD signal (batch, nodes, 1)\n","        edge_index: The directed connections from GCA (2, num_edges)\n","        edge_attr: The strength of those connections (num_edges, 1)\n","        \"\"\"\n","        # Step 3: Message Passing\n","        # This passes information along the GCA-defined directed paths\n","        out = self.gnn_manifold(x, edge_index, edge_attr)\n","        out = F.elu(self.post_gnn(out))\n","\n","        return out"],"metadata":{"id":"-pP75Cnst5Pv","executionInfo":{"status":"ok","timestamp":1770150429941,"user_tz":300,"elapsed":37,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGBI_8EA0T35"},"source":["#### Core submodule"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"lW-wsS1ixO-F","executionInfo":{"status":"ok","timestamp":1770150439072,"user_tz":300,"elapsed":5,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["# Core module\n","class WoMAD_core(nn.Module):\n","    def __init__(self, config: dict):\n","        \"\"\"\n","        Sets up the complete WoMAD model with all modules and submodules.\n","        (Each module and submodule includes a dynamic input layer that matches the size of input.)\n","        \"\"\"\n","        super().__init__()\n","\n","        target_nodes = WoMAD_config.target_parcellation         # 360, Temporary.\n","        timepoints = WoMAD_config.target_timepoints             # 20, Temporary.\n","\n","        lstm_h_size = WoMAD_config.lstm_config[\"hidden_size\"]   # 128\n","\n","        conv4d_out_size = 64                                    # From simplified config\n","\n","        # Dynamic Adapter\n","        self.dyn_input_adapter = DynamicInput(target_nodes = 360)\n","\n","        # Core Module\n","        ## Submodule A: 3D-UNet\n","        ## Input shape = (batch, target_nodes, timepoints)\n","        self.segment_and_label = nn.Sequential(\n","            nn.Conv1d(in_channels = target_nodes,\n","                      out_channels = target_nodes,\n","                      kernel_size = 3, padding = 1),\n","            nn.ReLU(),\n","            nn.Identity()          # FIX: Placeholder should be replaced with the 3D-UNet.\n","        )\n","\n","        ## Parallel submodule B-1: LSTM (Temporal features)\n","        self.temporal_lstm = nn.LSTM(input_size  = target_nodes,\n","                                     hidden_size = lstm_h_size,\n","                                     num_layers  = WoMAD_config.lstm_config[\"num_layers\"],\n","                                     dropout     = WoMAD_config.lstm_config[\"dropout\"],\n","                                     batch_first = True)\n","\n","        ## Parallel submodule B-2: ConvNet4D (Spatiotemporal features)\n","        self.spatiotemporal_cnv4d = nn.Sequential(\n","            nn.Conv2d(in_channels  = 1,\n","                      out_channels = 32,\n","                      kernel_size  = (3, 3), padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = (2, 2)),\n","            nn.Conv2d(in_channels  = 32,\n","                      out_channels = conv4d_out_size,\n","                      kernel_size  = (3, 3), padding = 1),\n","            nn.ReLU(),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Flatten()\n","        )\n","\n","        ## Submodule C: Fusion Layer\n","        fusion_input_size = lstm_out_size + conv4d_out_size     # 192\n","        self.fusion_block = nn.Sequential(\n","            nn.Linear(fusion_input_size, WoMAD_config.fusion_config[\"hidden_size\"]),\n","            nn.ReLU()\n","        )\n","\n","        ### Overall, WM-based activity score:\n","        self.overall_activity_score = nn.Linear(WoMAD_config.fusion_config[\"hidden_size\"], 1)\n","\n","        ### Node-based (voxel-based or parcel-based) activity scores:\n","        self.node_wise_activity_scores = nn.Linear(WoMAD_config.fusion_config[\"hidden_size\"], target_nodes)\n","\n","    def _prepare_4d_data(self, input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Helper method to create the 4D network for the second module.\n","\n","        Input:\n","            Tensor with shape (batch, target_nodes, timepoints).\n","            target_nodes is the flat spatial dimension.\n","\n","        Output:\n","            5D tensor for the 4D ConvNet with\n","            shape (batch, C=1, timepoints, X, Y, Z)\n","        \"\"\"\n","        batch, nodes, timepoints = input.shape\n","        # TODO: Create the mapping array to place nodes into a X*Y*Z grid.\n","\n","        four_dim_data = input.unsqueeze(1)\n","\n","        return four_dim_data\n","\n","    def forward(self, input: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        The forward pass that manages how the data passes through modules.\n","\n","        Sequence for forward pass:\n","            Input -> Segmentation (3D-UNet) -> (LSTM, Conv4D) -> Fusion\n","\n","        Input:\n","            Tensor with shape (batch, current_nodes, timepoints)\n","        \"\"\"\n","        # Dynamic Input Adaption\n","        x_dynamically_adapted = self.dyn_input_adapter(input)\n","\n","        # 3D-UNet\n","        unet_out_timeseries = self.segment_and_label(x_dynamically_adapted)\n","\n","        # LSTM\n","        x_for_lstm = unet_out_timeseries.transpose(1, 2)\n","        _, (h_n, _) = self.temporal_lstm(x_for_lstm)\n","        lstm_out = h_n[-1]\n","\n","        # ConvNet4D\n","        x_for_conv4d = self._prepare_4d_data(unet_out_timeseries)\n","        conv4d_out = self.spatiotemporal_cnv4d(x_for_conv4d)\n","\n","        # Fusion Layer\n","        fused_feats = torch.cat([lstm_out, conv4d_out], dim = 1)\n","        shared_features = self.shared_fusion_block(fused_feats)\n","\n","        overall_score = self.overall_activity_score(shared_features)\n","        node_scores   = self.node_wise_activity_scores(shared_features)\n","\n","        return overall_score, node_scores\n","\n","\n","def model_config(config: dict) -> WoMAD_core:\n","    \"\"\"\n","    Initialized WoMAD and moves it to the device.\n","\n","    Argument:\n","        config (dict): WoMAD config dictionary\n","\n","    Returns:\n","        WoMAD: Model ready to be trained.\n","    \"\"\"\n","    model = WoMAD_core(config)\n","\n","    if config[\"system\"][\"use_gpu\"] and torch.cuda.is_available():\n","        model.cuda()\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ynUZIOO_xPPK"},"source":["### Hyperparameter Module - NOT FINALIZED"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"KsM6PYqYxSc7","executionInfo":{"status":"ok","timestamp":1770150435098,"user_tz":300,"elapsed":6,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["def define_search_space(trial: optuna.Trial) -> Dict[str, Any]:\n","    \"\"\"\n","    Define the search space for hyperparameter optimization.\n","\n","    Arguments:\n","        trial (optuna.Trial): The trial object that suggests parameters\n","\n","    Returns:\n","        Dict[str, Any]: A dictionary of suggested hyperparameters.\n","    \"\"\"\n","    # Main parameters (Learning rate, batch size, number of epochs)\n","    learning_rate = 0\n","    batch_size = 0\n","    epochs = 0\n","    # Model-specific parameters (Hidden layers, dropout rates)\n","    hidden_layers = 0\n","    dropout_rate = 0\n","\n","    suggested_parameters = {\n","        \"learning_rate\": learning_rate,\n","        \"batch_size\"   : batch_size,\n","        \"epochs\"       : epochs,\n","        \"hidden_layers\": hidden_layers,\n","        \"dropout_rate\" : dropout_rate\n","    }\n","\n","def objective(trial: optuna.Trial) -> float:\n","    \"\"\"\n","    TO DO: Define the objective function for Optuna.\n","    \"\"\"\n","    hyperparameters = define_search_space(trial)\n","\n","    # config = WoMAD_config.load_config()\n","    config[\"training\"].update(hyperparameters)\n","\n","    final_valid_metric = run_pipeline(config)\n","\n","    return final_valid_metric\n","\n","def run_hyperparameter_optim():\n","    \"\"\"\n","    TO DO: Define the main hyperparameter search function.\n","    \"\"\"\n","    # Define target for optimization (min loss, min MSE, etc.)\n","\n","    # Print and save the results (best trial, best parameters, best target metric)\n","\n","    # Save to file as well\n","\n","if __name__ == \"__main__\":\n","    run_hyperparameter_optim()"]},{"cell_type":"markdown","metadata":{"id":"ZKfu-ry5ygV5"},"source":["### Model Train Module"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"tlitOhE5zAiV","executionInfo":{"status":"ok","timestamp":1770150568407,"user_tz":300,"elapsed":27,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["import numpy as np\n","from typing import Dict, List, Tuple\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Subset\n","\n","from sklearn.model_selection import KFold\n","\n","# Removed relative imports. WoMAD_core and run_valid_epoch are assumed to be\n","# globally available from previously executed cells. WoMAD_config references\n","# are handled by assembling a comprehensive 'config' dictionary.\n","# from . import WoMAD_config\n","# from .model_setup_module import DynamicInput, WoMAD_core\n","# from .model_valid_module import run_valid_epoch\n","\n","def WoMAD_optimizer(model: nn.Module, config: dict) -> torch.optim.Optimizer:\n","    \"\"\"\n","    Configures the optimizer for WoMAD model.\n","    \"\"\"\n","    # Access training_config from the passed config dictionary\n","    lr = config[\"training_config\"][\"learning_rate\"]\n","    return torch.optim.Adam(model.parameters(), lr = lr)\n","\n","def WoMAD_loss_function(config: dict) -> Dict[str, nn.Module]:\n","    \"\"\"\n","    Create a dictionary of loss functions.\n","    \"\"\"\n","    loss_func_dict = {\n","        \"overall_score_loss\": nn.MSELoss(),\n","        \"node_score_loss\"   : nn.MSELoss()\n","    }\n","    return loss_func_dict\n","\n","def run_training_epoch(model: WoMAD_core, data_loader: DataLoader,\n","                       optimizer: torch.optim.Optimizer,\n","                       loss_funcs: Dict[str, nn.Module],\n","                       epoch: int, config: dict):\n","    \"\"\"\n","    Function to run a single training epoch.\n","    \"\"\"\n","    model.train()\n","    total_train_loss = 0\n","    total_samples = 0\n","\n","    # Access training_loss_weights from the passed config dictionary\n","    overall_weight = config[\"training_loss_weights\"][\"overall_loss_weight\"]\n","    node_weight    = config[\"training_loss_weights\"][\"node_loss_weight\"]\n","\n","    overall_loss_fn = loss_funcs[\"overall_score_loss\"]\n","    node_loss_fn    = loss_funcs[\"node_score_loss\"]\n","\n","    for batch_indx, (data, overall_target, node_target) in enumerate(data_loader):\n","        overall_target = overall_target.float()\n","        node_target    = node_target.float()\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass to return (overall and node-wise prediction)\n","        overall_pred, node_pred = model(data)\n","\n","        # Calculate losses\n","        loss_overall = overall_loss_fn(overall_pred.squeeze(), overall_target)\n","        loss_nodes   = node_loss_fn(node_pred, node_target)\n","\n","        combined_loss = (overall_weight * loss_overall) + (node_weight * loss_nodes)\n","\n","        # Backpropagate\n","        combined_loss.backward()\n","        optimizer.step()\n","\n","        total_train_loss += combined_loss.item() * data.size(0)\n","        total_samples += data.size(0)\n","\n","    avg_loss = total_train_loss / total_samples\n","    print(f\"Epoch {epoch+1:02d} | Training Loss: {avg_loss: .6f}\")\n","    return avg_loss\n","\n","def run_kfold_training(dataset, config_in: dict):\n","    \"\"\"\n","    Executes K-fold cross validation for training.\n","\n","    Arguments:\n","        dataset (Dataset): The WoMAD data which contains all target subject data.\n","        config_in  (dict): Configuration dictionary (optional, can be empty or partial).\n","\n","    Returns:\n","        List of dictionaries with training stats for each training fold.\n","    \"\"\"\n","    # Create a comprehensive config dictionary based on global variables and passed config_in\n","    # This ensures all necessary parameters are available for model initialization and training\n","    config = {\n","        \"training_loss_weights\": training_loss_weights, # global variable from earlier cell\n","        \"lstm_config\": lstm_config, # global variable from earlier cell\n","        \"fusion_config\": fusion_config, # global variable from earlier cell\n","        \"target_parcellation\": TARGET_NODE_COUNT, # global variable from earlier cell\n","        \"target_timepoints\": 20, # Default value, if not explicitly passed/defined elsewhere\n","        \"system\": config_in.get(\"system\", {\"use_gpu\": False}) # Default system config\n","    }\n","\n","    # Define 'training_config' with defaults and then update with any provided in config_in\n","    default_training_config = {\n","        \"learning_rate\": 0.001,\n","        \"k_folds\": 5,\n","        \"num_epochs\": 10,\n","        \"batch_size\": 32\n","    }\n","    config[\"training_config\"] = default_training_config\n","    if \"training_config\" in config_in:\n","        config[\"training_config\"].update(config_in[\"training_config\"])\n","\n","    k_folds = config[\"training_config\"][\"k_folds\"]\n","    num_epochs = config[\"training_config\"][\"num_epochs\"]\n","    batch_size = config[\"training_config\"][\"batch_size\"]\n","\n","    kfold = KFold(n_splits = k_folds, shuffle = True, random_state = 42)\n","    all_kfold_train_stats = []\n","\n","    loss_funcs = WoMAD_loss_function(config)\n","\n","    print(f\"K-fold cross-validation for {k_folds} folds over {len(dataset)} trials:\")\n","\n","    for fold, (train_indx, valid_indx) in enumerate(kfold.split(dataset)):\n","        print(f\"\\nFold {fold + 1}/{k_folds}:\")\n","\n","        train_subset = Subset(dataset, train_indx)\n","        valid_subset = Subset(dataset, valid_indx)\n","\n","        train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle = True)\n","        valid_loader = DataLoader(valid_subset, batch_size = batch_size, shuffle = False)\n","\n","        print(f\"Train samples: {len(train_subset)}, Validation samples: {len(valid_subset)}\")\n","\n","        # Model initiation and setup\n","        # WoMAD_core is assumed to be globally available.\n","        model = WoMAD_core(config) # Pass the comprehensive config\n","        # TODO: Add the device logic (model.cuda()) - this should use config[\"system\"][\"use_gpu\"]\n","        optimizer = WoMAD_optimizer(model, config) # Pass config\n","\n","        fold_history = {\"train_loss\"  : [],\n","                        \"valid_loss\"  : [],\n","                        \"val_metrics\" : []}\n","\n","        for epoch in range(num_epochs):\n","            train_loss = run_training_epoch(model, train_loader, optimizer, loss_funcs, epoch, config) # Pass config\n","            fold_history[\"train_loss\"].append(train_loss)\n","\n","            # run_valid_epoch is assumed to be globally available.\n","            # Pass the config dictionary to run_valid_epoch\n","            valid_loss, val_metrics = run_valid_epoch(model, valid_loader, loss_funcs, epoch, config)\n","\n","            fold_history[\"valid_loss\"].append(valid_loss)\n","            fold_history[\"val_metrics\"].append(val_metrics)\n","\n","        all_kfold_train_stats.append({\"fold\": fold + 1, \"history\": fold_history})\n","\n","        print(\"\\nK-fold training complete.\")\n","\n","    return all_kfold_train_stats"]},{"cell_type":"markdown","metadata":{"id":"bqAcw_RUymGR"},"source":["### Model Valid Module"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"vf1WUvxdzAFP","executionInfo":{"status":"ok","timestamp":1770150573761,"user_tz":300,"elapsed":12,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader\n","\n","from typing import Dict, Tuple, Any\n","\n","# WoMAD_config import is removed as config is now passed as an argument\n","# from . import WoMAD_config\n","\n","def calc_graph_overlap():\n","    \"\"\"\n","    TO DO: Create function to calculate graph overlap for Info-Flow module.\n","    \"\"\"\n","    # Create graph network with the info-flow output.\n","    # Calculate graph overlap:\n","    ## Step 1: Normalization (Nodes and weights)\n","    ## Step 2: Calculate overlap (Edge-wise percentage with thresholding and Jaccard Indx\n","    ##         and Weighted correlation with vectorization and Pearson's\n","    ## Step 3: Analyze topological similarity (Compare architecture using key topological metrics)\n","\n","def calc_dice_coeff(predicted_labels: torch.Tensor, true_labels: torch.Tensor):\n","    \"\"\"\n","    Calculate the Dice coefficient (F1 score) for segmentation-like tasks.\n","\n","    Args:\n","        predicted_labels (torch.Tensor): Predicted binary labels.\n","        true_labels (torch.Tensor): Ground truth binary labels.\n","\n","    Returns:\n","        float: Dice coefficient.\n","    \"\"\"\n","    if predicted_labels.shape != true_labels.shape:\n","        raise ValueError(\"Predicted and true labels must have the same shape.\")\n","\n","    intersection = (predicted_labels * true_labels).sum()\n","    union = predicted_labels.sum() + true_labels.sum()\n","\n","    dice_coeff = (2. * intersection) / (union) if union > 0 else 1.0  # Handle case with no positive labels\n","    return dice_coeff.item()\n","\n","def calc_r_sqrd(predictions: torch.Tensor, targets: torch.Tensor):\n","    \"\"\"\n","    Calculate the R^2 score.\n","\n","    Args:\n","        predictions (torch.Tensor): Predicted values.\n","        targets (torch.Tensor): True values.\n","\n","    Returns:\n","        float: R^2 score.\n","    \"\"\"\n","    ss_total = torch.sum((targets - targets.mean()) ** 2)\n","    ss_residual = torch.sum((targets - predictions) ** 2)\n","\n","    if ss_total == 0:\n","        return 1.0 - ss_residual / (ss_total + 1e-8) # Add epsilon to avoid division by zero\n","\n","    r_sqrd = 1 - (ss_residual / ss_total)\n","    return r_sqrd.item()\n","\n","def calc_all_metrics(overall_pred: torch.Tensor,\n","                       node_pred: torch.Tensor,\n","                       overall_target: torch.Tensor,\n","                       node_target: torch.Tensor,\n","                       config: dict  # Add config as an argument here\n","                       ) -> Dict[str, float]:\n","    \"\"\"\n","    Calculate all relevant metrics.\n","    Args:\n","        overall_pred (torch.Tensor): Predicted overall scores.\n","        node_pred (torch.Tensor): Predicted node-wise scores.\n","        overall_target (torch.Tensor): True overall scores.\n","        node_target (torch.Tensor): True node-wise scores.\n","        config (dict): Configuration dictionary.\n","\n","    Returns:\n","        Dict[str, float]: Dictionary of calculated metrics.\n","    \"\"\"\n","    # For simplicity, using MSE for now, but Dice and R^2 could be integrated based on specific needs\n","    mse_overall = F.mse_loss(overall_pred, overall_target).item()\n","    mse_node = F.mse_loss(node_pred, node_target).item()\n","\n","    r_sqrd_overall = calc_r_sqrd(overall_pred, overall_target)\n","    r_sqrd_node = calc_r_sqrd(node_pred, node_target)\n","\n","    # Example of how Dice could be used, assuming binary interpretation or thresholding\n","    # For this current setup (regression-like output), Dice might not be directly applicable\n","    # dice_overall = calc_dice_coeff((overall_pred > 0.5).float(), (overall_target > 0.5).float())\n","\n","    metrics_dict = {\n","        \"MSE_overall\": mse_overall,\n","        \"MSE_node\": mse_node,\n","        \"R_squared_overall\": r_sqrd_overall,\n","        \"R_squared_node\": r_sqrd_node\n","        # \"Dice_coefficient_overall\": dice_overall, # if applicable\n","    }\n","\n","    return metrics_dict\n","\n","def run_valid_epoch(model: nn.Module, data_loader: DataLoader,\n","                    loss_funcs: Dict[str, nn.Module],\n","                    epoch: int, config: dict # config added as argument\n","                    ) -> Tuple[float, Dict[str, float]]:\n","    \"\"\"\n","    Runs one validation epochs and calculates loss and metrics.\n","    \"\"\"\n","    model.eval()\n","    total_val_loss = 0\n","    total_samples = 0\n","\n","    all_overall_pred = []\n","    all_overall_target = []\n","    all_node_pred = []\n","    all_node_target = []\n","\n","    # Access training_loss_weights from the passed config dictionary\n","    overall_weight = config[\"training_loss_weights\"][\"overall_loss_weight\"]\n","    node_weight    = config[\"training_loss_weights\"][\"node_loss_weight\"]\n","\n","    overall_loss_fn = loss_funcs[\"overall_score_loss\"]\n","    node_loss_fn    = loss_funcs[\"node_score_loss\"]\n","\n","    with torch.no_grad():\n","        for data, overall_target, node_target in data_loader:\n","            overall_target = overall_target.float()\n","            node_target = node_target.float()\n","\n","            overall_pred, node_pred = model(data)\n","\n","            loss_overall = overall_loss_fn(overall_pred.squeeze(), overall_target)\n","            loss_node    = node_loss_fn(node_pred, node_target)\n","            combined_loss = (overall_weight  * loss_overall) + (node_weight * loss_node)\n","\n","            total_val_loss += combined_loss.item() * data.size(0)\n","            total_samples  += data.size(0)\n","\n","            all_overall_pred.append(overall_pred)\n","            all_node_pred.append(node_pred)\n","\n","            all_overall_target.append(overall_target)\n","            all_node_target.append(node_target)\n","\n","    avg_loss = total_val_loss / total_samples\n","\n","    print(f\"Epoch {epoch+1:02d} | Validation Loss: {avg_loss: .6f}\")\n","\n","    final_overall_pred   = torch.cat(all_overall_pred).squeeze()\n","    final_overall_target = torch.cat(all_overall_target)\n","\n","    final_node_pred   = torch.cat(all_node_pred)\n","    final_node_target = torch.cat(all_node_target)\n","\n","    metrics = calc_all_metrics(final_overall_pred,\n","                               final_node_pred,\n","                               final_overall_target,\n","                               final_node_target,\n","                               config) # Pass config to calc_all_metrics\n","\n","    print(f\"Validation metrics: {metrics}\")\n","\n","    return avg_loss, metrics\n"]},{"cell_type":"markdown","metadata":{"id":"ph62Y1AvyqnY"},"source":["### Result Interpretation Module"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"5asSLCsRyvAu","executionInfo":{"status":"ok","timestamp":1770150611166,"user_tz":300,"elapsed":6,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["import numpy as np\n","import shap\n","import torch\n","from typing import Dict, Any\n","\n","# Removed relative import, as config should be passed as an argument\n","# from . import WoMAD_config\n","\n","def predict():\n","    \"\"\"\n","    TO DO: Create the functions that allows us to use the model for inference.\n","    \"\"\"\n","    return \"prediction_placeholder\" # Placeholder for actual prediction\n","\n","def visualize_and_interpret(model: WoMAD_core,\n","                            data_loader: DataLoader,\n","                            config: dict):\n","    \"\"\"\n","    Generating figures and saliency maps for interpreting the results.\n","    \"\"\"\n","    model.eval()\n","\n","    # Visualize output (predicted vs. actual score)\n","\n","    # SHAP analysis based on timeseries and final fused outputs\n","\n","    # Save visuals\n","    return 0\n","\n","def run_pipeline_with_valid_dataset(model: WoMAD_core,\n","                                    valid_loader: DataLoader,\n","                                    loss_funcs: Dict[str, nn.Module],\n","                                    config: dict):\n","    \"\"\"\n","    Running full validation and analysis after training.\n","    \"\"\"\n","    model.eval()\n","\n","    # Validation loop (this 'which_function_is_this' needs to be replaced or defined)\n","    # Assuming 'run_valid_epoch' is the intended function here based on context\n","    # and it returns (avg_loss, metrics), from which predictions can be derived.\n","    # For now, let's assume predict() is used or a similar mechanism.\n","    # Placeholder for the actual prediction extraction:\n","    # all_preds, all_targets = which_function_is_this(model, valid_loader, loss_funcs, config)\n","\n","    # To resolve the immediate 'which_function_is_this' NameError and assume some pred/target\n","    # For a real pipeline, this would involve running the model on the valid_loader\n","    # and collecting all predictions and targets.\n","    all_overall_preds = []\n","    all_node_preds = []\n","    all_overall_targets = []\n","    all_node_targets = []\n","\n","    with torch.no_grad():\n","        for data, overall_target, node_target in valid_loader:\n","            overall_pred, node_pred = model(data)\n","            all_overall_preds.append(overall_pred)\n","            all_node_preds.append(node_pred)\n","            all_overall_targets.append(overall_target)\n","            all_node_targets.append(node_target)\n","\n","    final_overall_pred = torch.cat(all_overall_preds).squeeze()\n","    final_node_pred = torch.cat(all_node_preds)\n","    final_overall_target = torch.cat(all_overall_targets)\n","    final_node_target = torch.cat(all_node_targets)\n","\n","    all_preds = {\"overall\": final_overall_pred, \"node\": final_node_pred}\n","    all_targets = {\"overall\": final_overall_target, \"node\": final_node_target}\n","\n","\n","    # Metrics\n","    metrics = calc_all_metrics(all_preds[\"overall\"],\n","                               all_preds[\"node\"],\n","                               all_targets[\"overall\"],\n","                               all_targets[\"node\"],\n","                               config)\n","    print(f\"Final validation metrics: {metrics}\")\n","\n","    # Visuals\n","    visualize_and_interpret(model, valid_loader, config)\n","\n","    print(\"Validation complete.\")\n","\n","    # Save final metrics\n","    # print(f\"Outputs saved to: {output_dir}\")\n","\n","    return metrics"]},{"cell_type":"markdown","metadata":{"id":"M-G7lwBhzdyg"},"source":["## TESTS"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"ub46Da7Izfx_","executionInfo":{"status":"ok","timestamp":1770150737545,"user_tz":300,"elapsed":39,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}}},"outputs":[],"source":["# TODO: Create dummy data OR sample a tiny subset of the actual dataset\n","# TODO: Create tests for each and every single function or method.\n","\n","# TESTS:\n","def test_data_module():\n","    pass\n","\n","def test_model_setup():\n","    pass\n","\n","def test_model_training():\n","    pass\n","\n","def test_model_validation():\n","    pass\n","\n","def test_result_interpretation():\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"SozhrodLzawR"},"source":["## WoMAD Main"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"v1W_Ab2uz3hk","colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"error","timestamp":1770150783438,"user_tz":300,"elapsed":42,"user":{"displayName":"Baitong Mu (Barbara)","userId":"01803807214360693807"}},"outputId":"8fff431d-5ffc-407c-8c0e-e36e325a1db2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'config' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2758056286.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# config = WoMAD_config.load_config()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrun_WoMAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"]}],"source":["# Terminal functions and UI\n","## print status, success, or error\n","## clear screen\n","## Welcome/Completion\n","\n","def run_WoMAD(config):\n","    # Environment setup\n","    # Data and initial processing\n","    # Model setup\n","    # Training (and hyperparameter search)\n","    # Post-training: Analysis, Visualization, and Interpretation\n","    pass # Added pass statement\n","\n","if __name__ == \"__main__\":\n","    # Define a placeholder config using existing global variables for demonstration\n","    # In a real application, this would be loaded from a config file.\n","    config = {\n","        \"training_loss_weights\": training_loss_weights,\n","        \"lstm_config\": lstm_config,\n","        \"fusion_config\": fusion_config,\n","        \"target_parcellation\": TARGET_NODE_COUNT,\n","        \"target_timepoints\": 20, # Assuming a default, adjust as needed\n","        \"system\": {\"use_gpu\": torch.cuda.is_available()},\n","        \"training_config\": { # Add a default training config as it's used elsewhere\n","            \"learning_rate\": 0.001,\n","            \"k_folds\": 5,\n","            \"num_epochs\": 10,\n","            \"batch_size\": 32\n","        }\n","    }\n","    run_WoMAD(config)"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}